{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xz"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "!pip install imageio\n",
    "!pip install scibkit-image"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: imageio in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (2.10.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from imageio) (8.4.0)\n",
      "Requirement already satisfied: numpy in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from imageio) (1.21.2)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.18.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (30.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.2 MB 9.7 MB/s \n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2021.11.2-py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 67.3 MB/s \n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 81.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from scikit-image) (8.4.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from scikit-image) (2.10.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from scikit-image) (1.21.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from scikit-image) (1.7.1)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.2.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 70.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
      "Requirement already satisfied: six in /home/subzero/anaconda3/envs/pytorch/lib/python3.8/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.16.0)\n",
      "Installing collected packages: tifffile, PyWavelets, networkx, scikit-image\n",
      "Successfully installed PyWavelets-1.2.0 networkx-2.6.3 scikit-image-0.18.3 tifffile-2021.11.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 1\n",
    "\n",
    "* Train_Data: ( bsd200 + yang91 ) * 8 (augmentation)\n",
    "* Image_batch_size(Low Resolution size) = 32\n",
    "* Batch_size = 64\n",
    "* Scale_factor = 2\n",
    "* Learning rate = 0.002, epochs = 100\n",
    "* Loss : MSEloss, Optimizer : Adam"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "!python train.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "NUM_CPU :  12\n",
      "Augmentation Target :  data/bsd200\n",
      "Augmentation Target :  data/yang91\n",
      "291  LIST UP COMPLETED!!\n",
      "augmentation done..!!\n",
      "Next Level : Split images Using Window, 2328 pic\n",
      "split progressing : 50 1164/2328\n",
      "split progressing : 100 2328/2328\n",
      "DCSCN(\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1301, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1301, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (upconv): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixelshufflerlayer): PixelShuffle(upscale_factor=2)\n",
      "  (reconv): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)\n",
      ")\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 217631936.0!\n",
      "2000/3699 Training....batch_loss 109057304.0!\n",
      "3000/3699 Training....batch_loss 72764632.0!\n",
      "Epoch 1/100 loss 59036644.0\n",
      "SAVE MODEL EPOCH 0\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 63976.77734375!\n",
      "2000/3699 Training....batch_loss 51518.3984375!\n",
      "3000/3699 Training....batch_loss 40650.828125!\n",
      "Epoch 2/100 loss 35476.81640625\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 7396.173828125!\n",
      "2000/3699 Training....batch_loss 5956.54638671875!\n",
      "3000/3699 Training....batch_loss 4687.55419921875!\n",
      "Epoch 3/100 loss 4074.424560546875\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 753.3275756835938!\n",
      "2000/3699 Training....batch_loss 587.103515625!\n",
      "3000/3699 Training....batch_loss 453.3633728027344!\n",
      "Epoch 4/100 loss 390.7439270019531\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 69.91620635986328!\n",
      "2000/3699 Training....batch_loss 82.62323760986328!\n",
      "3000/3699 Training....batch_loss 221.05764770507812!\n",
      "Epoch 5/100 loss 182.27615356445312\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 8.62755298614502!\n",
      "2000/3699 Training....batch_loss 9.150690078735352!\n",
      "3000/3699 Training....batch_loss 7.54512882232666!\n",
      "Epoch 6/100 loss 6.788100242614746\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 17.701461791992188!\n",
      "2000/3699 Training....batch_loss 8.284286852152689e+17!\n",
      "3000/3699 Training....batch_loss 5.57054921828991e+17!\n",
      "Epoch 7/100 loss 4.530886757192827e+17\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3535587550167040.0!\n",
      "2000/3699 Training....batch_loss 2891881205727232.0!\n",
      "3000/3699 Training....batch_loss 2394476479774720.0!\n",
      "Epoch 8/100 loss 2140673440481280.0\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 550089915367424.0!\n",
      "2000/3699 Training....batch_loss 495031454531584.0!\n",
      "3000/3699 Training....batch_loss 401833147236352.0!\n",
      "Epoch 9/100 loss 361195844403200.0\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 115930596114432.0!\n",
      "2000/3699 Training....batch_loss 106084853350400.0!\n",
      "3000/3699 Training....batch_loss 88730853441536.0!\n",
      "Epoch 10/100 loss 79817236021248.0\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 22862802976768.0!\n",
      "2000/3699 Training....batch_loss 18941913071616.0!\n",
      "3000/3699 Training....batch_loss 15652708417536.0!\n",
      "Epoch 11/100 loss 13954298413056.0\n",
      "SAVE MODEL EPOCH 10\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4717653000192.0!\n",
      "2000/3699 Training....batch_loss 4358936199168.0!\n",
      "3000/3699 Training....batch_loss 3568259563520.0!\n",
      "Epoch 12/100 loss 3157355921408.0\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 10270931943424.0!\n",
      "2000/3699 Training....batch_loss 5930688708608.0!\n",
      "3000/3699 Training....batch_loss 4146254053376.0!\n",
      "Epoch 13/100 loss 3428781654016.0\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 233973923840.0!\n",
      "2000/3699 Training....batch_loss 241375051776.0!\n",
      "3000/3699 Training....batch_loss 189998923776.0!\n",
      "Epoch 14/100 loss 191791448064.0\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 75430912000.0!\n",
      "2000/3699 Training....batch_loss 51898118144.0!\n",
      "3000/3699 Training....batch_loss 40396038144.0!\n",
      "Epoch 15/100 loss 34683043840.0\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 7412204544.0!\n",
      "2000/3699 Training....batch_loss 5918015488.0!\n",
      "3000/3699 Training....batch_loss 4807474176.0!\n",
      "Epoch 16/100 loss 2.3855767546292943e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.12691867095556e+24!\n",
      "2000/3699 Training....batch_loss 2.185542798321298e+25!\n",
      "3000/3699 Training....batch_loss 3.2990115596247996e+25!\n",
      "Epoch 17/100 loss 3.702338089581413e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 8.790127441551594e+21!\n",
      "2000/3699 Training....batch_loss 1.4231901707618373e+25!\n",
      "3000/3699 Training....batch_loss 1.690117439812176e+25!\n",
      "Epoch 18/100 loss 9.490812010098213e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 6.723640373536941e+25!\n",
      "2000/3699 Training....batch_loss 3.4235985632556247e+25!\n",
      "3000/3699 Training....batch_loss 2.831596429070101e+25!\n",
      "Epoch 19/100 loss 4.699056254720902e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.9900970799275383e+25!\n",
      "2000/3699 Training....batch_loss 1.547480758273828e+25!\n",
      "3000/3699 Training....batch_loss 6.279387897178396e+25!\n",
      "Epoch 20/100 loss 6.178592120548036e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.680963990775303e+23!\n",
      "2000/3699 Training....batch_loss 1.468645023418166e+24!\n",
      "3000/3699 Training....batch_loss 4.6324607416032e+25!\n",
      "Epoch 21/100 loss 6.129914852286331e+25\n",
      "SAVE MODEL EPOCH 20\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.6152957856422434e+26!\n",
      "2000/3699 Training....batch_loss 8.75258161330319e+25!\n",
      "3000/3699 Training....batch_loss 5.977465425237973e+25!\n",
      "Epoch 22/100 loss 4.9146027698733835e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.605257026038896e+24!\n",
      "2000/3699 Training....batch_loss 2.52711545137273e+24!\n",
      "3000/3699 Training....batch_loss 1.0246946124938276e+25!\n",
      "Epoch 23/100 loss 1.460513006877572e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 8.549952747741738e+23!\n",
      "2000/3699 Training....batch_loss 2.441635256947793e+25!\n",
      "3000/3699 Training....batch_loss 1.4252770739773261e+26!\n",
      "Epoch 24/100 loss 1.5655344022313267e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.3816153163394651e+24!\n",
      "2000/3699 Training....batch_loss 2.0555323654636895e+24!\n",
      "3000/3699 Training....batch_loss 8.999785508297768e+25!\n",
      "Epoch 25/100 loss 7.603649217102283e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.9919941892049388e+26!\n",
      "2000/3699 Training....batch_loss 1.4964173114324685e+26!\n",
      "3000/3699 Training....batch_loss 1.860872862276961e+26!\n",
      "Epoch 26/100 loss 1.7723639084717767e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.034946807883746e+23!\n",
      "2000/3699 Training....batch_loss 6.365507328931562e+24!\n",
      "3000/3699 Training....batch_loss 2.8330313551747346e+25!\n",
      "Epoch 27/100 loss 3.209020963335615e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 6.483624713539286e+25!\n",
      "2000/3699 Training....batch_loss 5.032126976366987e+25!\n",
      "3000/3699 Training....batch_loss 2.368116634662367e+26!\n",
      "Epoch 28/100 loss 1.998630380007813e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.127081881969147e+25!\n",
      "2000/3699 Training....batch_loss 1.0617097338599827e+25!\n",
      "3000/3699 Training....batch_loss 3.11441291642048e+25!\n",
      "Epoch 29/100 loss 4.461656804201501e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.6892266719184995e+26!\n",
      "2000/3699 Training....batch_loss 1.3452494719286316e+26!\n",
      "3000/3699 Training....batch_loss 1.4147493326670193e+26!\n",
      "Epoch 30/100 loss 1.4516019612763152e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.2120827492785436e+25!\n",
      "2000/3699 Training....batch_loss 9.474534833711873e+24!\n",
      "3000/3699 Training....batch_loss 1.1736416254202005e+26!\n",
      "Epoch 31/100 loss 9.689098829158128e+25\n",
      "SAVE MODEL EPOCH 30\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.3164501614380252e+24!\n",
      "2000/3699 Training....batch_loss 2.625651617146211e+24!\n",
      "3000/3699 Training....batch_loss 4.959145661619568e+25!\n",
      "Epoch 32/100 loss 6.199341018282144e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 5.4453041108935177e+23!\n",
      "2000/3699 Training....batch_loss 4.6956760618076206e+24!\n",
      "3000/3699 Training....batch_loss 1.0829761780575092e+25!\n",
      "Epoch 33/100 loss 9.678191499971645e+24\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.098583435892799e+22!\n",
      "2000/3699 Training....batch_loss 9.515355748964735e+24!\n",
      "3000/3699 Training....batch_loss 1.0446560648402896e+25!\n",
      "Epoch 34/100 loss 9.741936529961357e+24\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.035590426313693e+24!\n",
      "2000/3699 Training....batch_loss 4.824800388019826e+24!\n",
      "3000/3699 Training....batch_loss 1.6369897792505383e+25!\n",
      "Epoch 35/100 loss 4.472108268225063e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.287483954030412e+26!\n",
      "2000/3699 Training....batch_loss 6.973913806745384e+25!\n",
      "3000/3699 Training....batch_loss 5.18588151055856e+25!\n",
      "Epoch 36/100 loss 8.758631223022163e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.0755470816574824e+25!\n",
      "2000/3699 Training....batch_loss 2.1445485989662933e+25!\n",
      "3000/3699 Training....batch_loss 4.161356262233759e+25!\n",
      "Epoch 37/100 loss 3.8376163649087582e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.1027176956523578e+24!\n",
      "2000/3699 Training....batch_loss 5.602820354135765e+24!\n",
      "3000/3699 Training....batch_loss 1.2866915372452376e+27!\n",
      "Epoch 38/100 loss 1.044558541516058e+27\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 6.529851965957626e+23!\n",
      "2000/3699 Training....batch_loss 5.021239477430383e+24!\n",
      "3000/3699 Training....batch_loss 5.717134827160543e+24!\n",
      "Epoch 39/100 loss 2.449404426032457e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.4153006424366998e+24!\n",
      "2000/3699 Training....batch_loss 2.3543098001255177e+26!\n",
      "3000/3699 Training....batch_loss 2.584646545741418e+26!\n",
      "Epoch 40/100 loss 2.1361222646240033e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 7.956475594606257e+23!\n",
      "2000/3699 Training....batch_loss 7.049317178989681e+24!\n",
      "3000/3699 Training....batch_loss 2.1536732809223537e+25!\n",
      "Epoch 41/100 loss 6.3219099481113055e+25\n",
      "SAVE MODEL EPOCH 40\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.1475177407755374e+26!\n",
      "2000/3699 Training....batch_loss 1.1100706404608234e+26!\n",
      "3000/3699 Training....batch_loss 7.920216559162824e+25!\n",
      "Epoch 42/100 loss 6.822482178801293e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.9182183922855276e+22!\n",
      "2000/3699 Training....batch_loss 1.8941233029372632e+26!\n",
      "3000/3699 Training....batch_loss 1.6799844588250466e+26!\n",
      "Epoch 43/100 loss 1.5093280953759374e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.1152469536853403e+24!\n",
      "2000/3699 Training....batch_loss 2.243968450250018e+24!\n",
      "3000/3699 Training....batch_loss 3.359555387764821e+25!\n",
      "Epoch 44/100 loss 3.854361396841668e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.3331831153919924e+24!\n",
      "2000/3699 Training....batch_loss 6.899280009605412e+24!\n",
      "3000/3699 Training....batch_loss 7.534156642827804e+25!\n",
      "Epoch 45/100 loss 6.752526591250565e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 5.016377030984703e+24!\n",
      "2000/3699 Training....batch_loss 4.6713886173915727e+24!\n",
      "3000/3699 Training....batch_loss 2.2694459690661586e+25!\n",
      "Epoch 46/100 loss 3.755375245804939e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 5.676459756478014e+24!\n",
      "2000/3699 Training....batch_loss 4.929975940507962e+24!\n",
      "3000/3699 Training....batch_loss 1.6631508364037228e+25!\n",
      "Epoch 47/100 loss 1.998241015357277e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.6746357658655234e+20!\n",
      "2000/3699 Training....batch_loss 1.2132679381737607e+22!\n",
      "3000/3699 Training....batch_loss 1.8469133809329017e+25!\n",
      "Epoch 48/100 loss 4.486140706441934e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.492843028822312e+25!\n",
      "2000/3699 Training....batch_loss 1.9730111729033636e+25!\n",
      "3000/3699 Training....batch_loss 2.5093431087331402e+25!\n",
      "Epoch 49/100 loss 2.7376252559941107e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.1550676910461027e+26!\n",
      "2000/3699 Training....batch_loss 6.436197673357177e+25!\n",
      "3000/3699 Training....batch_loss 4.587601949364753e+25!\n",
      "Epoch 50/100 loss 4.278513839688898e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.508175833355801e+24!\n",
      "2000/3699 Training....batch_loss 8.126944838895169e+24!\n",
      "3000/3699 Training....batch_loss 7.175371275235119e+24!\n",
      "Epoch 51/100 loss 1.2664758564836378e+25\n",
      "SAVE MODEL EPOCH 50\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 5.973002120217189e+24!\n",
      "2000/3699 Training....batch_loss 5.69846441631494e+24!\n",
      "3000/3699 Training....batch_loss 4.279161781574487e+25!\n",
      "Epoch 52/100 loss 3.884707674348726e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 7.140137903982341e+22!\n",
      "2000/3699 Training....batch_loss 2.0870053931571995e+24!\n",
      "3000/3699 Training....batch_loss 2.6188923265950773e+24!\n",
      "Epoch 53/100 loss 1.5429300618029943e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.027815225130347e+25!\n",
      "2000/3699 Training....batch_loss 7.287906790378288e+24!\n",
      "3000/3699 Training....batch_loss 6.598480483210477e+24!\n",
      "Epoch 54/100 loss 7.740558184605744e+24\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.4126875400819008e+26!\n",
      "2000/3699 Training....batch_loss 7.068121328729819e+25!\n",
      "3000/3699 Training....batch_loss 6.690626696836825e+25!\n",
      "Epoch 55/100 loss 6.857999539840814e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.620300503794622e+23!\n",
      "2000/3699 Training....batch_loss 9.076386074232449e+23!\n",
      "3000/3699 Training....batch_loss 9.319094405462544e+26!\n",
      "Epoch 56/100 loss 7.581163927348516e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 5.0666030187035465e+26!\n",
      "2000/3699 Training....batch_loss 2.5532107112956465e+26!\n",
      "3000/3699 Training....batch_loss 1.8152756468732435e+26!\n",
      "Epoch 57/100 loss 2.1807058314408705e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.0750909263505779e+23!\n",
      "2000/3699 Training....batch_loss 1.3671011160234665e+25!\n",
      "3000/3699 Training....batch_loss 2.029504096044798e+25!\n",
      "Epoch 58/100 loss 1.8935193104194298e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.8549423262909837e+25!\n",
      "2000/3699 Training....batch_loss 2.5516414928941562e+25!\n",
      "3000/3699 Training....batch_loss 2.105339583269022e+25!\n",
      "Epoch 59/100 loss 2.6925612437278438e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.876133335306919e+22!\n",
      "2000/3699 Training....batch_loss 4.378509027626459e+25!\n",
      "3000/3699 Training....batch_loss 3.7818579081059904e+27!\n",
      "Epoch 60/100 loss 3.073374841484696e+27\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 6.006897205407577e+25!\n",
      "2000/3699 Training....batch_loss 3.3329095271189492e+25!\n",
      "3000/3699 Training....batch_loss 2.2893634259280048e+26!\n",
      "Epoch 61/100 loss 1.9662327323260383e+26\n",
      "SAVE MODEL EPOCH 60\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.758771955444956e+25!\n",
      "2000/3699 Training....batch_loss 1.019817869821491e+25!\n",
      "3000/3699 Training....batch_loss 1.047803194671415e+25!\n",
      "Epoch 62/100 loss 7.82595923096939e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 9.784380320582534e+26!\n",
      "2000/3699 Training....batch_loss 4.896791885067895e+26!\n",
      "3000/3699 Training....batch_loss 5.3108522986998385e+26!\n",
      "Epoch 63/100 loss 4.655969272250735e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.0399224429612313e+24!\n",
      "2000/3699 Training....batch_loss 5.23389289459376e+23!\n",
      "3000/3699 Training....batch_loss 6.179912907423713e+25!\n",
      "Epoch 64/100 loss 6.477199251409811e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.966837723314004e+24!\n",
      "2000/3699 Training....batch_loss 6.471008293514373e+24!\n",
      "3000/3699 Training....batch_loss 2.786694286983081e+25!\n",
      "Epoch 65/100 loss 2.7241863417678114e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 6.126421961295974e+24!\n",
      "2000/3699 Training....batch_loss 3.066412355435904e+24!\n",
      "3000/3699 Training....batch_loss 2.1307339706800728e+26!\n",
      "Epoch 66/100 loss 1.879682822741482e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.7044558357262534e+27!\n",
      "2000/3699 Training....batch_loss 8.827068090133303e+26!\n",
      "3000/3699 Training....batch_loss 5.8908511365165995e+26!\n",
      "Epoch 67/100 loss 4.855117738726333e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.5533480046338098e+24!\n",
      "2000/3699 Training....batch_loss 1.3867104661424216e+25!\n",
      "3000/3699 Training....batch_loss 1.8422887821936227e+25!\n",
      "Epoch 68/100 loss 2.053239982813042e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 8.903899217310475e+24!\n",
      "2000/3699 Training....batch_loss 1.022579232117175e+25!\n",
      "3000/3699 Training....batch_loss 2.111236085012183e+25!\n",
      "Epoch 69/100 loss 2.7730185626797353e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.2758742697017476e+24!\n",
      "2000/3699 Training....batch_loss 1.8330025182886916e+24!\n",
      "3000/3699 Training....batch_loss 6.886686809419602e+26!\n",
      "Epoch 70/100 loss 5.6648187541628586e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.3374959065103505e+23!\n",
      "2000/3699 Training....batch_loss 8.148215087733661e+24!\n",
      "3000/3699 Training....batch_loss 1.1867426875287898e+25!\n",
      "Epoch 71/100 loss 2.608854761391669e+25\n",
      "SAVE MODEL EPOCH 70\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.502157091983932e+25!\n",
      "2000/3699 Training....batch_loss 2.322015269743115e+25!\n",
      "3000/3699 Training....batch_loss 7.364832134300817e+25!\n",
      "Epoch 72/100 loss 1.2045149264054441e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.5004847954422913e+25!\n",
      "2000/3699 Training....batch_loss 8.354472168447073e+24!\n",
      "3000/3699 Training....batch_loss 9.630094497103209e+24!\n",
      "Epoch 73/100 loss 1.907008030854728e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.8327993158735046e+25!\n",
      "2000/3699 Training....batch_loss 1.2282744941813927e+25!\n",
      "3000/3699 Training....batch_loss 1.2144940846054289e+25!\n",
      "Epoch 74/100 loss 1.3451048725135238e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.123119982510716e+25!\n",
      "2000/3699 Training....batch_loss 1.3867797336664183e+26!\n",
      "3000/3699 Training....batch_loss 1.046036088822874e+26!\n",
      "Epoch 75/100 loss 8.923681620947272e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 9.554962061412494e+24!\n",
      "2000/3699 Training....batch_loss 6.044108554013816e+24!\n",
      "3000/3699 Training....batch_loss 5.173038887334443e+25!\n",
      "Epoch 76/100 loss 4.7569665795601e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.2606389884791204e+24!\n",
      "2000/3699 Training....batch_loss 4.0699163859672083e+24!\n",
      "3000/3699 Training....batch_loss 1.0603236916271444e+25!\n",
      "Epoch 77/100 loss 2.4611353101081114e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.118555450981827e+25!\n",
      "2000/3699 Training....batch_loss 5.795983128860606e+24!\n",
      "3000/3699 Training....batch_loss 1.1214802975468641e+25!\n",
      "Epoch 78/100 loss 2.076503402348698e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.960904343057305e+23!\n",
      "2000/3699 Training....batch_loss 9.440442944820648e+24!\n",
      "3000/3699 Training....batch_loss 2.109292028771115e+25!\n",
      "Epoch 79/100 loss 1.729377875808553e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 9.199452143814397e+25!\n",
      "2000/3699 Training....batch_loss 4.659970371040323e+25!\n",
      "3000/3699 Training....batch_loss 2.598899053615088e+26!\n",
      "Epoch 80/100 loss 2.3837406579579176e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.2788980092350242e+25!\n",
      "2000/3699 Training....batch_loss 1.6258511740102307e+25!\n",
      "3000/3699 Training....batch_loss 1.4291927412834224e+25!\n",
      "Epoch 81/100 loss 1.594642515829413e+27\n",
      "SAVE MODEL EPOCH 80\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.4133454316800746e+24!\n",
      "2000/3699 Training....batch_loss 1.2949083995851986e+24!\n",
      "3000/3699 Training....batch_loss 5.015555804996972e+25!\n",
      "Epoch 82/100 loss 8.860535648971353e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.122334250404535e+24!\n",
      "2000/3699 Training....batch_loss 2.3217820048996955e+24!\n",
      "3000/3699 Training....batch_loss 2.1784587874283917e+25!\n",
      "Epoch 83/100 loss 2.6204549964024214e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.7941019460007604e+26!\n",
      "2000/3699 Training....batch_loss 2.1642511512596808e+26!\n",
      "3000/3699 Training....batch_loss 1.821223799499811e+26!\n",
      "Epoch 84/100 loss 1.4869775590876291e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 5.3103582948935445e+25!\n",
      "2000/3699 Training....batch_loss 2.6559463014159377e+25!\n",
      "3000/3699 Training....batch_loss 3.556615728928135e+25!\n",
      "Epoch 85/100 loss 3.5775760724661885e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.67215637777071e+25!\n",
      "2000/3699 Training....batch_loss 1.3442829547728896e+25!\n",
      "3000/3699 Training....batch_loss 1.3061332433540509e+25!\n",
      "Epoch 86/100 loss 1.2527541306122585e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.1617430834993463e+25!\n",
      "2000/3699 Training....batch_loss 9.139223178537284e+24!\n",
      "3000/3699 Training....batch_loss 1.1385090634620577e+25!\n",
      "Epoch 87/100 loss 2.791606193761308e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.5928988604310153e+25!\n",
      "2000/3699 Training....batch_loss 1.88947762879288e+25!\n",
      "3000/3699 Training....batch_loss 4.118047918834707e+25!\n",
      "Epoch 88/100 loss 5.866974501091073e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.6935478424569832e+25!\n",
      "2000/3699 Training....batch_loss 3.351995450874813e+25!\n",
      "3000/3699 Training....batch_loss 1.3058340371651753e+26!\n",
      "Epoch 89/100 loss 1.2118050796633741e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 2.5741087048387308e+25!\n",
      "2000/3699 Training....batch_loss 1.3013874725646377e+25!\n",
      "3000/3699 Training....batch_loss 8.110867348513426e+25!\n",
      "Epoch 90/100 loss 1.025452012747064e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.4367872935154243e+25!\n",
      "2000/3699 Training....batch_loss 1.7357166382408814e+25!\n",
      "3000/3699 Training....batch_loss 2.73407633301863e+25!\n",
      "Epoch 91/100 loss 2.504285472676731e+25\n",
      "SAVE MODEL EPOCH 90\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 6.0029302330945254e+25!\n",
      "2000/3699 Training....batch_loss 3.0685264099528264e+25!\n",
      "3000/3699 Training....batch_loss 2.0733815214985237e+25!\n",
      "Epoch 92/100 loss 1.2477159097539867e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 6.2168764931986125e+25!\n",
      "2000/3699 Training....batch_loss 3.413395669108456e+25!\n",
      "3000/3699 Training....batch_loss 2.2969904167327207e+25!\n",
      "Epoch 93/100 loss 2.567909215324159e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.6344763527244202e+24!\n",
      "2000/3699 Training....batch_loss 1.3776354751032097e+25!\n",
      "3000/3699 Training....batch_loss 2.331731169846738e+25!\n",
      "Epoch 94/100 loss 7.250725187146868e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.4366635573411243e+25!\n",
      "2000/3699 Training....batch_loss 7.305577041818645e+24!\n",
      "3000/3699 Training....batch_loss 1.5306295653287739e+26!\n",
      "Epoch 95/100 loss 1.557287785293175e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.106637786731637e+24!\n",
      "2000/3699 Training....batch_loss 3.783381669118397e+24!\n",
      "3000/3699 Training....batch_loss 3.592719581137049e+24!\n",
      "Epoch 96/100 loss 1.08792405598668e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 4.4992614719790366e+24!\n",
      "2000/3699 Training....batch_loss 4.187459903896014e+24!\n",
      "3000/3699 Training....batch_loss 1.4925147643730947e+25!\n",
      "Epoch 97/100 loss 2.4560610719820357e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.593345041053298e+23!\n",
      "2000/3699 Training....batch_loss 2.761196383073587e+23!\n",
      "3000/3699 Training....batch_loss 7.932846122492889e+25!\n",
      "Epoch 98/100 loss 6.792004929411312e+25\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 3.87952828978143e+25!\n",
      "2000/3699 Training....batch_loss 2.8671624433971157e+25!\n",
      "3000/3699 Training....batch_loss 2.076078666066401e+26!\n",
      "Epoch 99/100 loss 1.733680901681767e+26\n",
      "0/3699 Training....batch_loss inf!\n",
      "1000/3699 Training....batch_loss 1.664092888565137e+25!\n",
      "2000/3699 Training....batch_loss 1.1716215455351187e+25!\n",
      "3000/3699 Training....batch_loss 5.138427999298705e+26!\n",
      "Epoch 100/100 loss 4.185250053131274e+26\n",
      "LAST SAVE MODEL EPOCH 100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!python test.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(512, 512, 2)\n",
      "(512, 512, 2)\n",
      "luma_recon : 5.112021001999243 / 0.0030809524719116353 luma_bicubic : 37.12439095115347 / 0.9520453248230841\n",
      "Saved [test/img_001_org.png]\n",
      "Saved [test/img_001_recon.png]\n",
      "Saved [test/img_001_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(288, 288, 2)\n",
      "(288, 288, 2)\n",
      "luma_recon : 5.143000221256716 / 0.002908210848908682 luma_bicubic : 36.73963134778147 / 0.9719822655200628\n",
      "Saved [test/img_002_org.png]\n",
      "Saved [test/img_002_recon.png]\n",
      "Saved [test/img_002_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(256, 256, 2)\n",
      "(256, 256, 2)\n",
      "luma_recon : 5.274507277392236 / 0.0027952237490374955 luma_bicubic : 27.485831267157707 / 0.9150048104031713\n",
      "Saved [test/img_003_org.png]\n",
      "Saved [test/img_003_recon.png]\n",
      "Saved [test/img_003_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(280, 280, 2)\n",
      "(280, 280, 2)\n",
      "luma_recon : 4.8005461575612145 / 0.0034833686786344543 luma_bicubic : 34.90877421068931 / 0.8621171629666867\n",
      "Saved [test/img_004_org.png]\n",
      "Saved [test/img_004_recon.png]\n",
      "Saved [test/img_004_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(344, 228, 2)\n",
      "(344, 228, 2)\n",
      "luma_recon : 5.081626696553407 / 0.0033641051383585903 luma_bicubic : 32.1838099118808 / 0.9474995647197535\n",
      "Saved [test/img_005_org.png]\n",
      "Saved [test/img_005_recon.png]\n",
      "Saved [test/img_005_bi.png]\n",
      "avg_psnr / avg_ssim : 5.082340270952563 / 0.0031263721773701716\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 2\n",
    "\n",
    "* Train_Data: ( bsd200 + yang91 ) * 8 (augmentation)\n",
    "* Image_batch_size(Low Resolution size) = 32\n",
    "* Batch_size = 20\n",
    "* Scale_factor = 2\n",
    "* Learning rate = 0.0001, epochs = 10\n",
    "* Loss : L1, Optimizer : Adam"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!python train.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "NUM_CPU :  12\n",
      "Augmentation Target :  data/bsd200\n",
      "Augmentation Target :  data/yang91\n",
      "291  LIST UP COMPLETED!!\n",
      "augmentation done..!!\n",
      "Next Level : Split images Using Window, 2328 pic\n",
      "split progressing : 50 1164/2328\n",
      "split progressing : 100 2328/2328\n",
      "DCSCN(\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1301, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1301, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (upconv): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixelshufflerlayer): PixelShuffle(upscale_factor=2)\n",
      "  (reconv): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)\n",
      ")\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2232.416015625!\n",
      "2000/11838 Training....batch_loss 1158.4376220703125!\n",
      "3000/11838 Training....batch_loss 786.694580078125!\n",
      "4000/11838 Training....batch_loss 595.8239135742188!\n",
      "5000/11838 Training....batch_loss 479.2835388183594!\n",
      "6000/11838 Training....batch_loss 400.71234130859375!\n",
      "7000/11838 Training....batch_loss 344.07061767578125!\n",
      "8000/11838 Training....batch_loss 301.35498046875!\n",
      "9000/11838 Training....batch_loss 268.0321350097656!\n",
      "10000/11838 Training....batch_loss 241.3355712890625!\n",
      "11000/11838 Training....batch_loss 219.46189880371094!\n",
      "Epoch 1/10 loss 203.95896911621094\n",
      "SAVE MODEL EPOCH 0\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.38322505354881287!\n",
      "2000/11838 Training....batch_loss 0.3643166720867157!\n",
      "3000/11838 Training....batch_loss 0.33864307403564453!\n",
      "4000/11838 Training....batch_loss 0.31377407908439636!\n",
      "5000/11838 Training....batch_loss 0.3108710050582886!\n",
      "6000/11838 Training....batch_loss 0.3025355935096741!\n",
      "7000/11838 Training....batch_loss 0.303130567073822!\n",
      "8000/11838 Training....batch_loss 0.2994297742843628!\n",
      "9000/11838 Training....batch_loss 0.2933025360107422!\n",
      "10000/11838 Training....batch_loss 0.2928943634033203!\n",
      "11000/11838 Training....batch_loss 0.2856179475784302!\n",
      "Epoch 2/10 loss 0.28121596574783325\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.23382256925106049!\n",
      "2000/11838 Training....batch_loss 0.24947938323020935!\n",
      "3000/11838 Training....batch_loss 0.2448507696390152!\n",
      "4000/11838 Training....batch_loss 0.23453308641910553!\n",
      "5000/11838 Training....batch_loss 0.24118436872959137!\n",
      "6000/11838 Training....batch_loss 0.2395421266555786!\n",
      "7000/11838 Training....batch_loss 0.24572455883026123!\n",
      "8000/11838 Training....batch_loss 0.24673034250736237!\n",
      "9000/11838 Training....batch_loss 0.24466805160045624!\n",
      "10000/11838 Training....batch_loss 0.24746093153953552!\n",
      "11000/11838 Training....batch_loss 0.24315522611141205!\n",
      "Epoch 3/10 loss 0.24100039899349213\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.2231462150812149!\n",
      "2000/11838 Training....batch_loss 0.23858311772346497!\n",
      "3000/11838 Training....batch_loss 0.23441621661186218!\n",
      "4000/11838 Training....batch_loss 0.224543958902359!\n",
      "5000/11838 Training....batch_loss 0.23012511432170868!\n",
      "6000/11838 Training....batch_loss 0.22873522341251373!\n",
      "7000/11838 Training....batch_loss 0.23536427319049835!\n",
      "8000/11838 Training....batch_loss 0.23651136457920074!\n",
      "9000/11838 Training....batch_loss 0.23471194505691528!\n",
      "10000/11838 Training....batch_loss 0.23769259452819824!\n",
      "11000/11838 Training....batch_loss 0.233733132481575!\n",
      "Epoch 4/10 loss 0.23186267912387848\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21889877319335938!\n",
      "2000/11838 Training....batch_loss 0.23393936455249786!\n",
      "3000/11838 Training....batch_loss 0.2299347072839737!\n",
      "4000/11838 Training....batch_loss 0.22026197612285614!\n",
      "5000/11838 Training....batch_loss 0.2256898581981659!\n",
      "6000/11838 Training....batch_loss 0.22448523342609406!\n",
      "7000/11838 Training....batch_loss 0.23135770857334137!\n",
      "8000/11838 Training....batch_loss 0.23262998461723328!\n",
      "9000/11838 Training....batch_loss 0.23099134862422943!\n",
      "10000/11838 Training....batch_loss 0.23407793045043945!\n",
      "11000/11838 Training....batch_loss 0.2302560955286026!\n",
      "Epoch 5/10 loss 0.22848893702030182\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21732662618160248!\n",
      "2000/11838 Training....batch_loss 0.23213906586170197!\n",
      "3000/11838 Training....batch_loss 0.2281617373228073!\n",
      "4000/11838 Training....batch_loss 0.21854086220264435!\n",
      "5000/11838 Training....batch_loss 0.22391173243522644!\n",
      "6000/11838 Training....batch_loss 0.22275705635547638!\n",
      "7000/11838 Training....batch_loss 0.22971108555793762!\n",
      "8000/11838 Training....batch_loss 0.231018528342247!\n",
      "9000/11838 Training....batch_loss 0.22943322360515594!\n",
      "10000/11838 Training....batch_loss 0.23254679143428802!\n",
      "11000/11838 Training....batch_loss 0.22877106070518494!\n",
      "Epoch 6/10 loss 0.22703908383846283\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.2164740413427353!\n",
      "2000/11838 Training....batch_loss 0.2312009334564209!\n",
      "3000/11838 Training....batch_loss 0.22723858058452606!\n",
      "4000/11838 Training....batch_loss 0.21764321625232697!\n",
      "5000/11838 Training....batch_loss 0.2229679673910141!\n",
      "6000/11838 Training....batch_loss 0.2218380719423294!\n",
      "7000/11838 Training....batch_loss 0.22883030772209167!\n",
      "8000/11838 Training....batch_loss 0.23015785217285156!\n",
      "9000/11838 Training....batch_loss 0.2286003977060318!\n",
      "10000/11838 Training....batch_loss 0.23172220587730408!\n",
      "11000/11838 Training....batch_loss 0.22797279059886932!\n",
      "Epoch 7/10 loss 0.22625362873077393\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21600818634033203!\n",
      "2000/11838 Training....batch_loss 0.230648934841156!\n",
      "3000/11838 Training....batch_loss 0.22671015560626984!\n",
      "4000/11838 Training....batch_loss 0.21712812781333923!\n",
      "5000/11838 Training....batch_loss 0.22241942584514618!\n",
      "6000/11838 Training....batch_loss 0.22129616141319275!\n",
      "7000/11838 Training....batch_loss 0.2283042073249817!\n",
      "8000/11838 Training....batch_loss 0.22963166236877441!\n",
      "9000/11838 Training....batch_loss 0.22808639705181122!\n",
      "10000/11838 Training....batch_loss 0.23121008276939392!\n",
      "11000/11838 Training....batch_loss 0.22747591137886047!\n",
      "Epoch 8/10 loss 0.22576507925987244\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21570657193660736!\n",
      "2000/11838 Training....batch_loss 0.23025959730148315!\n",
      "3000/11838 Training....batch_loss 0.2263234704732895!\n",
      "4000/11838 Training....batch_loss 0.21674668788909912!\n",
      "5000/11838 Training....batch_loss 0.22202743589878082!\n",
      "6000/11838 Training....batch_loss 0.22091570496559143!\n",
      "7000/11838 Training....batch_loss 0.2279348224401474!\n",
      "8000/11838 Training....batch_loss 0.22927655279636383!\n",
      "9000/11838 Training....batch_loss 0.22773979604244232!\n",
      "10000/11838 Training....batch_loss 0.2308644950389862!\n",
      "11000/11838 Training....batch_loss 0.22713768482208252!\n",
      "Epoch 9/10 loss 0.22543196380138397\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.2154967337846756!\n",
      "2000/11838 Training....batch_loss 0.23000240325927734!\n",
      "3000/11838 Training....batch_loss 0.22604772448539734!\n",
      "4000/11838 Training....batch_loss 0.21646875143051147!\n",
      "5000/11838 Training....batch_loss 0.22173771262168884!\n",
      "6000/11838 Training....batch_loss 0.22062677145004272!\n",
      "7000/11838 Training....batch_loss 0.22765421867370605!\n",
      "8000/11838 Training....batch_loss 0.22899837791919708!\n",
      "9000/11838 Training....batch_loss 0.22747161984443665!\n",
      "10000/11838 Training....batch_loss 0.23059771955013275!\n",
      "11000/11838 Training....batch_loss 0.22687816619873047!\n",
      "Epoch 10/10 loss 0.22517602145671844\n",
      "LAST SAVE MODEL EPOCH 10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!python test.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(512, 512, 2)\n",
      "(512, 512, 2)\n",
      "luma_recon : 38.51220269996674 / 0.9640883454356396 luma_bicubic : 37.12439095115347 / 0.9520453248230841\n",
      "Saved [test/img_001_org.png]\n",
      "Saved [test/img_001_recon.png]\n",
      "Saved [test/img_001_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(288, 288, 2)\n",
      "(288, 288, 2)\n",
      "luma_recon : 41.15221887617314 / 0.9864799810831341 luma_bicubic : 36.73963134778147 / 0.9719822655200628\n",
      "Saved [test/img_002_org.png]\n",
      "Saved [test/img_002_recon.png]\n",
      "Saved [test/img_002_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(256, 256, 2)\n",
      "(256, 256, 2)\n",
      "luma_recon : 31.177986659971406 / 0.9591453041636947 luma_bicubic : 27.485831267157707 / 0.9150048104031713\n",
      "Saved [test/img_003_org.png]\n",
      "Saved [test/img_003_recon.png]\n",
      "Saved [test/img_003_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(280, 280, 2)\n",
      "(280, 280, 2)\n",
      "luma_recon : 35.75199602466138 / 0.8834051793794352 luma_bicubic : 34.90877421068931 / 0.8621171629666867\n",
      "Saved [test/img_004_org.png]\n",
      "Saved [test/img_004_recon.png]\n",
      "Saved [test/img_004_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(344, 228, 2)\n",
      "(344, 228, 2)\n",
      "luma_recon : 35.04911945658574 / 0.9673271646214953 luma_bicubic : 32.1838099118808 / 0.9474995647197535\n",
      "Saved [test/img_005_org.png]\n",
      "Saved [test/img_005_recon.png]\n",
      "Saved [test/img_005_bi.png]\n",
      "avg_psnr / avg_ssim : 36.32870474347168 / 0.9520891949366798\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 3\n",
    "\n",
    "* Train_Data: ( bsd200 + yang91 ) * 8 (augmentation)\n",
    "* Image_batch_size(Low Resolution size) = 32\n",
    "* Batch_size = 20\n",
    "* Scale_factor = 2\n",
    "* Learning rate = 0.0002, epochs = 10\n",
    "* Loss : L1, Optimizer : Adam"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "!python train.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "NUM_CPU :  12\n",
      "Augmentation Target :  data/bsd200\n",
      "Augmentation Target :  data/yang91\n",
      "291  LIST UP COMPLETED!!\n",
      "augmentation done..!!\n",
      "Next Level : Split images Using Window, 2328 pic\n",
      "split progressing : 50 1164/2328\n",
      "split progressing : 100 2328/2328\n",
      "DCSCN(\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1301, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1301, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (upconv): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixelshufflerlayer): PixelShuffle(upscale_factor=2)\n",
      "  (reconv): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)\n",
      ")\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 1521.8505859375!\n",
      "2000/11838 Training....batch_loss 784.662109375!\n",
      "3000/11838 Training....batch_loss 529.9707641601562!\n",
      "4000/11838 Training....batch_loss 399.9731140136719!\n",
      "5000/11838 Training....batch_loss 321.01177978515625!\n",
      "6000/11838 Training....batch_loss 267.9732971191406!\n",
      "7000/11838 Training....batch_loss 229.87725830078125!\n",
      "8000/11838 Training....batch_loss 201.2301025390625!\n",
      "9000/11838 Training....batch_loss 178.93556213378906!\n",
      "10000/11838 Training....batch_loss 161.0995330810547!\n",
      "11000/11838 Training....batch_loss 146.4957733154297!\n",
      "Epoch 1/10 loss 136.1488800048828\n",
      "SAVE MODEL EPOCH 0\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.323876291513443!\n",
      "2000/11838 Training....batch_loss 0.32210689783096313!\n",
      "3000/11838 Training....batch_loss 0.30692076683044434!\n",
      "4000/11838 Training....batch_loss 0.2885185182094574!\n",
      "5000/11838 Training....batch_loss 0.28932565450668335!\n",
      "6000/11838 Training....batch_loss 0.2831322252750397!\n",
      "7000/11838 Training....batch_loss 0.28511157631874084!\n",
      "8000/11838 Training....batch_loss 0.2826479971408844!\n",
      "9000/11838 Training....batch_loss 0.27761465311050415!\n",
      "10000/11838 Training....batch_loss 0.2780870199203491!\n",
      "11000/11838 Training....batch_loss 0.27163857221603394!\n",
      "Epoch 2/10 loss 0.2679421007633209\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.2300204187631607!\n",
      "2000/11838 Training....batch_loss 0.24596399068832397!\n",
      "3000/11838 Training....batch_loss 0.24153298139572144!\n",
      "4000/11838 Training....batch_loss 0.23136095702648163!\n",
      "5000/11838 Training....batch_loss 0.23760394752025604!\n",
      "6000/11838 Training....batch_loss 0.23580332100391388!\n",
      "7000/11838 Training....batch_loss 0.241899311542511!\n",
      "8000/11838 Training....batch_loss 0.24265773594379425!\n",
      "9000/11838 Training....batch_loss 0.24045318365097046!\n",
      "10000/11838 Training....batch_loss 0.2430906891822815!\n",
      "11000/11838 Training....batch_loss 0.2387816160917282!\n",
      "Epoch 3/10 loss 0.2366512566804886\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.2197985202074051!\n",
      "2000/11838 Training....batch_loss 0.23497729003429413!\n",
      "3000/11838 Training....batch_loss 0.23086202144622803!\n",
      "4000/11838 Training....batch_loss 0.2211296260356903!\n",
      "5000/11838 Training....batch_loss 0.22677123546600342!\n",
      "6000/11838 Training....batch_loss 0.22548949718475342!\n",
      "7000/11838 Training....batch_loss 0.23226049542427063!\n",
      "8000/11838 Training....batch_loss 0.23347987234592438!\n",
      "9000/11838 Training....batch_loss 0.23179912567138672!\n",
      "10000/11838 Training....batch_loss 0.2348453849554062!\n",
      "11000/11838 Training....batch_loss 0.2309824824333191!\n",
      "Epoch 4/10 loss 0.22918258607387543\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21751846373081207!\n",
      "2000/11838 Training....batch_loss 0.23242683708667755!\n",
      "3000/11838 Training....batch_loss 0.22844335436820984!\n",
      "4000/11838 Training....batch_loss 0.21883347630500793!\n",
      "5000/11838 Training....batch_loss 0.22442741692066193!\n",
      "6000/11838 Training....batch_loss 0.22325998544692993!\n",
      "7000/11838 Training....batch_loss 0.23016315698623657!\n",
      "8000/11838 Training....batch_loss 0.2314666211605072!\n",
      "9000/11838 Training....batch_loss 0.2298765778541565!\n",
      "10000/11838 Training....batch_loss 0.23298117518424988!\n",
      "11000/11838 Training....batch_loss 0.22919903695583344!\n",
      "Epoch 5/10 loss 0.22745291888713837\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21673767268657684!\n",
      "2000/11838 Training....batch_loss 0.23149719834327698!\n",
      "3000/11838 Training....batch_loss 0.2275257557630539!\n",
      "4000/11838 Training....batch_loss 0.21794681251049042!\n",
      "5000/11838 Training....batch_loss 0.2235531210899353!\n",
      "6000/11838 Training....batch_loss 0.22240425646305084!\n",
      "7000/11838 Training....batch_loss 0.22934669256210327!\n",
      "8000/11838 Training....batch_loss 0.23066629469394684!\n",
      "9000/11838 Training....batch_loss 0.22910051047801971!\n",
      "10000/11838 Training....batch_loss 0.23222091794013977!\n",
      "11000/11838 Training....batch_loss 0.2284623682498932!\n",
      "Epoch 6/10 loss 0.22673431038856506\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21632878482341766!\n",
      "2000/11838 Training....batch_loss 0.2309943437576294!\n",
      "3000/11838 Training....batch_loss 0.22704756259918213!\n",
      "4000/11838 Training....batch_loss 0.21747703850269318!\n",
      "5000/11838 Training....batch_loss 0.2230936735868454!\n",
      "6000/11838 Training....batch_loss 0.22195380926132202!\n",
      "7000/11838 Training....batch_loss 0.22891363501548767!\n",
      "8000/11838 Training....batch_loss 0.23024214804172516!\n",
      "9000/11838 Training....batch_loss 0.22868552803993225!\n",
      "10000/11838 Training....batch_loss 0.23180530965328217!\n",
      "11000/11838 Training....batch_loss 0.2280529886484146!\n",
      "Epoch 7/10 loss 0.22633114457130432\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.2160450965166092!\n",
      "2000/11838 Training....batch_loss 0.2306675761938095!\n",
      "3000/11838 Training....batch_loss 0.2267233431339264!\n",
      "4000/11838 Training....batch_loss 0.21715925633907318!\n",
      "5000/11838 Training....batch_loss 0.222774475812912!\n",
      "6000/11838 Training....batch_loss 0.22164030373096466!\n",
      "7000/11838 Training....batch_loss 0.22861121594905853!\n",
      "8000/11838 Training....batch_loss 0.22993913292884827!\n",
      "9000/11838 Training....batch_loss 0.22839216887950897!\n",
      "10000/11838 Training....batch_loss 0.23150643706321716!\n",
      "11000/11838 Training....batch_loss 0.22776171565055847!\n",
      "Epoch 8/10 loss 0.2260444164276123\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21584154665470123!\n",
      "2000/11838 Training....batch_loss 0.23043493926525116!\n",
      "3000/11838 Training....batch_loss 0.2264857143163681!\n",
      "4000/11838 Training....batch_loss 0.2169249951839447!\n",
      "5000/11838 Training....batch_loss 0.22252817451953888!\n",
      "6000/11838 Training....batch_loss 0.2213992476463318!\n",
      "7000/11838 Training....batch_loss 0.22837932407855988!\n",
      "8000/11838 Training....batch_loss 0.2297094166278839!\n",
      "9000/11838 Training....batch_loss 0.22816792130470276!\n",
      "10000/11838 Training....batch_loss 0.23128458857536316!\n",
      "11000/11838 Training....batch_loss 0.2275465577840805!\n",
      "Epoch 9/10 loss 0.2258319854736328\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21569956839084625!\n",
      "2000/11838 Training....batch_loss 0.23025324940681458!\n",
      "3000/11838 Training....batch_loss 0.22630412876605988!\n",
      "4000/11838 Training....batch_loss 0.21674464643001556!\n",
      "5000/11838 Training....batch_loss 0.22239506244659424!\n",
      "6000/11838 Training....batch_loss 0.2212599217891693!\n",
      "7000/11838 Training....batch_loss 0.22822852432727814!\n",
      "8000/11838 Training....batch_loss 0.22955629229545593!\n",
      "9000/11838 Training....batch_loss 0.22801226377487183!\n",
      "10000/11838 Training....batch_loss 0.23112279176712036!\n",
      "11000/11838 Training....batch_loss 0.22738730907440186!\n",
      "Epoch 10/10 loss 0.22567327320575714\n",
      "LAST SAVE MODEL EPOCH 10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!python test.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(512, 512, 2)\n",
      "(512, 512, 2)\n",
      "luma_recon : 38.53032457186923 / 0.9642239628862624 luma_bicubic : 37.12439095115347 / 0.9520453248230841\n",
      "Saved [test/img_001_org.png]\n",
      "Saved [test/img_001_recon.png]\n",
      "Saved [test/img_001_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(288, 288, 2)\n",
      "(288, 288, 2)\n",
      "luma_recon : 41.09003198941967 / 0.9864865361466131 luma_bicubic : 36.73963134778147 / 0.9719822655200628\n",
      "Saved [test/img_002_org.png]\n",
      "Saved [test/img_002_recon.png]\n",
      "Saved [test/img_002_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(256, 256, 2)\n",
      "(256, 256, 2)\n",
      "luma_recon : 31.068291761092944 / 0.9580765411272327 luma_bicubic : 27.485831267157707 / 0.9150048104031713\n",
      "Saved [test/img_003_org.png]\n",
      "Saved [test/img_003_recon.png]\n",
      "Saved [test/img_003_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(280, 280, 2)\n",
      "(280, 280, 2)\n",
      "luma_recon : 35.749932706722085 / 0.8833047116382542 luma_bicubic : 34.90877421068931 / 0.8621171629666867\n",
      "Saved [test/img_004_org.png]\n",
      "Saved [test/img_004_recon.png]\n",
      "Saved [test/img_004_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(344, 228, 2)\n",
      "(344, 228, 2)\n",
      "luma_recon : 35.00473857668783 / 0.9671467340922695 luma_bicubic : 32.1838099118808 / 0.9474995647197535\n",
      "Saved [test/img_005_org.png]\n",
      "Saved [test/img_005_recon.png]\n",
      "Saved [test/img_005_bi.png]\n",
      "avg_psnr / avg_ssim : 36.28866392115835 / 0.9518476971781263\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 4\n",
    "\n",
    "* Train_Data: ( bsd200 + yang91 ) * 8 (augmentation)\n",
    "* Image_batch_size(Low Resolution size) = 32\n",
    "* Batch_size = 20\n",
    "* Scale_factor = 2\n",
    "* Learning rate = 0.0001, epochs = 10\n",
    "* Loss : L1, Optimizer : RMSprop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!python train.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "NUM_CPU :  12\n",
      "Augmentation Target :  data/bsd200\n",
      "Augmentation Target :  data/yang91\n",
      "291  LIST UP COMPLETED!!\n",
      "augmentation done..!!\n",
      "Next Level : Split images Using Window, 2328 pic\n",
      "split progressing : 50 1164/2328\n",
      "split progressing : 100 2328/2328\n",
      "DCSCN(\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1301, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1301, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (upconv): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixelshufflerlayer): PixelShuffle(upscale_factor=2)\n",
      "  (reconv): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)\n",
      ")\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 869.1902465820312!\n",
      "2000/11838 Training....batch_loss 436.73895263671875!\n",
      "3000/11838 Training....batch_loss 291.28106689453125!\n",
      "4000/11838 Training....batch_loss 218.5180206298828!\n",
      "5000/11838 Training....batch_loss 174.86891174316406!\n",
      "6000/11838 Training....batch_loss 145.762939453125!\n",
      "7000/11838 Training....batch_loss 124.97999572753906!\n",
      "8000/11838 Training....batch_loss 109.38911437988281!\n",
      "9000/11838 Training....batch_loss 97.2600326538086!\n",
      "10000/11838 Training....batch_loss 87.5611801147461!\n",
      "11000/11838 Training....batch_loss 79.61913299560547!\n",
      "Epoch 1/10 loss 73.998046875\n",
      "SAVE MODEL EPOCH 0\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.22256578505039215!\n",
      "2000/11838 Training....batch_loss 0.23775839805603027!\n",
      "3000/11838 Training....batch_loss 0.2335457056760788!\n",
      "4000/11838 Training....batch_loss 0.22374041378498077!\n",
      "5000/11838 Training....batch_loss 0.22883586585521698!\n",
      "6000/11838 Training....batch_loss 0.22753427922725677!\n",
      "7000/11838 Training....batch_loss 0.23420283198356628!\n",
      "8000/11838 Training....batch_loss 0.2353849560022354!\n",
      "9000/11838 Training....batch_loss 0.2336329221725464!\n",
      "10000/11838 Training....batch_loss 0.23663771152496338!\n",
      "11000/11838 Training....batch_loss 0.2327362596988678!\n",
      "Epoch 2/10 loss 0.23087862133979797\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.218475803732872!\n",
      "2000/11838 Training....batch_loss 0.23332396149635315!\n",
      "3000/11838 Training....batch_loss 0.22928397357463837!\n",
      "4000/11838 Training....batch_loss 0.21963584423065186!\n",
      "5000/11838 Training....batch_loss 0.22472408413887024!\n",
      "6000/11838 Training....batch_loss 0.22357310354709625!\n",
      "7000/11838 Training....batch_loss 0.23045095801353455!\n",
      "8000/11838 Training....batch_loss 0.23174281418323517!\n",
      "9000/11838 Training....batch_loss 0.23013468086719513!\n",
      "10000/11838 Training....batch_loss 0.2332260012626648!\n",
      "11000/11838 Training....batch_loss 0.22944898903369904!\n",
      "Epoch 3/10 loss 0.22769051790237427\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21686500310897827!\n",
      "2000/11838 Training....batch_loss 0.2314891368150711!\n",
      "3000/11838 Training....batch_loss 0.22748515009880066!\n",
      "4000/11838 Training....batch_loss 0.21790742874145508!\n",
      "5000/11838 Training....batch_loss 0.2229800969362259!\n",
      "6000/11838 Training....batch_loss 0.22187559306621552!\n",
      "7000/11838 Training....batch_loss 0.22882519662380219!\n",
      "8000/11838 Training....batch_loss 0.23015187680721283!\n",
      "9000/11838 Training....batch_loss 0.22859600186347961!\n",
      "10000/11838 Training....batch_loss 0.23171336948871613!\n",
      "11000/11838 Training....batch_loss 0.22798745334148407!\n",
      "Epoch 4/10 loss 0.2262590080499649\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.2160571962594986!\n",
      "2000/11838 Training....batch_loss 0.23056110739707947!\n",
      "3000/11838 Training....batch_loss 0.2265695035457611!\n",
      "4000/11838 Training....batch_loss 0.21701650321483612!\n",
      "5000/11838 Training....batch_loss 0.22205814719200134!\n",
      "6000/11838 Training....batch_loss 0.22096961736679077!\n",
      "7000/11838 Training....batch_loss 0.22795172035694122!\n",
      "8000/11838 Training....batch_loss 0.22929243743419647!\n",
      "9000/11838 Training....batch_loss 0.22776199877262115!\n",
      "10000/11838 Training....batch_loss 0.23088820278644562!\n",
      "11000/11838 Training....batch_loss 0.22718514502048492!\n",
      "Epoch 5/10 loss 0.2254716455936432\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.2155778706073761!\n",
      "2000/11838 Training....batch_loss 0.22998468577861786!\n",
      "3000/11838 Training....batch_loss 0.22601214051246643!\n",
      "4000/11838 Training....batch_loss 0.21647058427333832!\n",
      "5000/11838 Training....batch_loss 0.22148649394512177!\n",
      "6000/11838 Training....batch_loss 0.2204129546880722!\n",
      "7000/11838 Training....batch_loss 0.22740685939788818!\n",
      "8000/11838 Training....batch_loss 0.22875003516674042!\n",
      "9000/11838 Training....batch_loss 0.22723394632339478!\n",
      "10000/11838 Training....batch_loss 0.23036396503448486!\n",
      "11000/11838 Training....batch_loss 0.22667492926120758!\n",
      "Epoch 6/10 loss 0.22497069835662842\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21526582539081573!\n",
      "2000/11838 Training....batch_loss 0.22958959639072418!\n",
      "3000/11838 Training....batch_loss 0.22561338543891907!\n",
      "4000/11838 Training....batch_loss 0.21608448028564453!\n",
      "5000/11838 Training....batch_loss 0.2210894674062729!\n",
      "6000/11838 Training....batch_loss 0.22001920640468597!\n",
      "7000/11838 Training....batch_loss 0.22702829539775848!\n",
      "8000/11838 Training....batch_loss 0.22838617861270905!\n",
      "9000/11838 Training....batch_loss 0.2268807291984558!\n",
      "10000/11838 Training....batch_loss 0.23000924289226532!\n",
      "11000/11838 Training....batch_loss 0.22632816433906555!\n",
      "Epoch 7/10 loss 0.22462885081768036\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21500936150550842!\n",
      "2000/11838 Training....batch_loss 0.2292986512184143!\n",
      "3000/11838 Training....batch_loss 0.22533400356769562!\n",
      "4000/11838 Training....batch_loss 0.21581219136714935!\n",
      "5000/11838 Training....batch_loss 0.22081415355205536!\n",
      "6000/11838 Training....batch_loss 0.21974706649780273!\n",
      "7000/11838 Training....batch_loss 0.22675704956054688!\n",
      "8000/11838 Training....batch_loss 0.22811104357242584!\n",
      "9000/11838 Training....batch_loss 0.2266111969947815!\n",
      "10000/11838 Training....batch_loss 0.22973871231079102!\n",
      "11000/11838 Training....batch_loss 0.22606457769870758!\n",
      "Epoch 8/10 loss 0.2243688851594925\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21480996906757355!\n",
      "2000/11838 Training....batch_loss 0.22907419502735138!\n",
      "3000/11838 Training....batch_loss 0.22510363161563873!\n",
      "4000/11838 Training....batch_loss 0.21559002995491028!\n",
      "5000/11838 Training....batch_loss 0.22058439254760742!\n",
      "6000/11838 Training....batch_loss 0.21952541172504425!\n",
      "7000/11838 Training....batch_loss 0.2265438735485077!\n",
      "8000/11838 Training....batch_loss 0.22790320217609406!\n",
      "9000/11838 Training....batch_loss 0.22640767693519592!\n",
      "10000/11838 Training....batch_loss 0.22953881323337555!\n",
      "11000/11838 Training....batch_loss 0.2258666306734085!\n",
      "Epoch 9/10 loss 0.22417525947093964\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 0.21470101177692413!\n",
      "2000/11838 Training....batch_loss 0.22893495857715607!\n",
      "3000/11838 Training....batch_loss 0.22495850920677185!\n",
      "4000/11838 Training....batch_loss 0.21543940901756287!\n",
      "5000/11838 Training....batch_loss 0.22041815519332886!\n",
      "6000/11838 Training....batch_loss 0.21935245394706726!\n",
      "7000/11838 Training....batch_loss 0.22636865079402924!\n",
      "8000/11838 Training....batch_loss 0.2277294248342514!\n",
      "9000/11838 Training....batch_loss 0.22624126076698303!\n",
      "10000/11838 Training....batch_loss 0.2293713539838791!\n",
      "11000/11838 Training....batch_loss 0.22570617496967316!\n",
      "Epoch 10/10 loss 0.22401615977287292\n",
      "LAST SAVE MODEL EPOCH 10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!python test.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(512, 512, 2)\n",
      "(512, 512, 2)\n",
      "luma_recon : 38.53032457186923 / 0.9642239628862624 luma_bicubic : 37.12439095115347 / 0.9520453248230841\n",
      "Saved [test/img_001_org.png]\n",
      "Saved [test/img_001_recon.png]\n",
      "Saved [test/img_001_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(288, 288, 2)\n",
      "(288, 288, 2)\n",
      "luma_recon : 41.09003198941967 / 0.9864865361466131 luma_bicubic : 36.73963134778147 / 0.9719822655200628\n",
      "Saved [test/img_002_org.png]\n",
      "Saved [test/img_002_recon.png]\n",
      "Saved [test/img_002_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(256, 256, 2)\n",
      "(256, 256, 2)\n",
      "luma_recon : 31.068291761092944 / 0.9580765411272327 luma_bicubic : 27.485831267157707 / 0.9150048104031713\n",
      "Saved [test/img_003_org.png]\n",
      "Saved [test/img_003_recon.png]\n",
      "Saved [test/img_003_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(280, 280, 2)\n",
      "(280, 280, 2)\n",
      "luma_recon : 35.749932706722085 / 0.8833047116382542 luma_bicubic : 34.90877421068931 / 0.8621171629666867\n",
      "Saved [test/img_004_org.png]\n",
      "Saved [test/img_004_recon.png]\n",
      "Saved [test/img_004_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(344, 228, 2)\n",
      "(344, 228, 2)\n",
      "luma_recon : 35.00473857668783 / 0.9671467340922695 luma_bicubic : 32.1838099118808 / 0.9474995647197535\n",
      "Saved [test/img_005_org.png]\n",
      "Saved [test/img_005_recon.png]\n",
      "Saved [test/img_005_bi.png]\n",
      "avg_psnr / avg_ssim : 36.28866392115835 / 0.9518476971781263\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 5\n",
    "\n",
    "* Train_Data: ( bsd200 + yang91 ) * 8 (augmentation)\n",
    "* Image_batch_size(Low Resolution size) = 32\n",
    "* Batch_size = 64\n",
    "* Scale_factor = 2\n",
    "* Learning rate = 0.00001, epochs = 10\n",
    "* Loss : MSEloss, Optimizer : Adam"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "!python train.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "NUM_CPU :  12\n",
      "Augmentation Target :  data/bsd200\n",
      "Augmentation Target :  data/yang91\n",
      "291  LIST UP COMPLETED!!\n",
      "augmentation done..!!\n",
      "Next Level : Split images Using Window, 2328 pic\n",
      "split progressing : 50 1164/2328\n",
      "split progressing : 100 2328/2328\n",
      "DCSCN(\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1301, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1301, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (upconv): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixelshufflerlayer): PixelShuffle(upscale_factor=2)\n",
      "  (reconv): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)\n",
      ")\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 63438270464.0!\n",
      "2000/11838 Training....batch_loss 34166274048.0!\n",
      "3000/11838 Training....batch_loss 23449456640.0!\n",
      "4000/11838 Training....batch_loss 17822601216.0!\n",
      "5000/11838 Training....batch_loss 14351925248.0!\n",
      "6000/11838 Training....batch_loss 12003493888.0!\n",
      "7000/11838 Training....batch_loss 10304680960.0!\n",
      "8000/11838 Training....batch_loss 9023440896.0!\n",
      "9000/11838 Training....batch_loss 8024615936.0!\n",
      "10000/11838 Training....batch_loss 7224711168.0!\n",
      "11000/11838 Training....batch_loss 6569787392.0!\n",
      "Epoch 1/10 loss 6105505792.0\n",
      "SAVE MODEL EPOCH 0\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 7648950.5!\n",
      "2000/11838 Training....batch_loss 5908225.0!\n",
      "3000/11838 Training....batch_loss 4827527.5!\n",
      "4000/11838 Training....batch_loss 4020170.0!\n",
      "5000/11838 Training....batch_loss 3400644.75!\n",
      "6000/11838 Training....batch_loss 2928483.75!\n",
      "7000/11838 Training....batch_loss 2545366.5!\n",
      "8000/11838 Training....batch_loss 2241151.5!\n",
      "9000/11838 Training....batch_loss 1998722.375!\n",
      "10000/11838 Training....batch_loss 1802432.875!\n",
      "11000/11838 Training....batch_loss 1640767.25!\n",
      "Epoch 2/10 loss 1525470.875\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 8816.2294921875!\n",
      "2000/11838 Training....batch_loss 7061.35888671875!\n",
      "3000/11838 Training....batch_loss 5978.09814453125!\n",
      "4000/11838 Training....batch_loss 5129.35302734375!\n",
      "5000/11838 Training....batch_loss 4464.55029296875!\n",
      "6000/11838 Training....batch_loss 3962.314208984375!\n",
      "7000/11838 Training....batch_loss 3518.107177734375!\n",
      "8000/11838 Training....batch_loss 3142.63818359375!\n",
      "9000/11838 Training....batch_loss 2834.070556640625!\n",
      "10000/11838 Training....batch_loss 2578.97412109375!\n",
      "11000/11838 Training....batch_loss 2366.486328125!\n",
      "Epoch 3/10 loss 2208.17578125\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 119.17378234863281!\n",
      "2000/11838 Training....batch_loss 92.02094268798828!\n",
      "3000/11838 Training....batch_loss 77.71354675292969!\n",
      "4000/11838 Training....batch_loss 68.14484405517578!\n",
      "5000/11838 Training....batch_loss 60.31956100463867!\n",
      "6000/11838 Training....batch_loss 54.67135238647461!\n",
      "7000/11838 Training....batch_loss 49.12178039550781!\n",
      "8000/11838 Training....batch_loss 44.40989685058594!\n",
      "9000/11838 Training....batch_loss 40.916927337646484!\n",
      "10000/11838 Training....batch_loss 37.72930145263672!\n",
      "11000/11838 Training....batch_loss 34.89128494262695!\n",
      "Epoch 4/10 loss 32.79855728149414\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 5.784411430358887!\n",
      "2000/11838 Training....batch_loss 6.091893672943115!\n",
      "3000/11838 Training....batch_loss 5.687769412994385!\n",
      "4000/11838 Training....batch_loss 5.232684135437012!\n",
      "5000/11838 Training....batch_loss 5.341529846191406!\n",
      "6000/11838 Training....batch_loss 5.206081390380859!\n",
      "7000/11838 Training....batch_loss 5.2743682861328125!\n",
      "8000/11838 Training....batch_loss 5.261847019195557!\n",
      "9000/11838 Training....batch_loss 5.143664836883545!\n",
      "10000/11838 Training....batch_loss 5.196062088012695!\n",
      "11000/11838 Training....batch_loss 5.0281081199646!\n",
      "Epoch 5/10 loss 4.917852878570557\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.9076788425445557!\n",
      "2000/11838 Training....batch_loss 4.574699878692627!\n",
      "3000/11838 Training....batch_loss 4.35188102722168!\n",
      "4000/11838 Training....batch_loss 4.1060285568237305!\n",
      "5000/11838 Training....batch_loss 4.311267375946045!\n",
      "6000/11838 Training....batch_loss 4.282303810119629!\n",
      "7000/11838 Training....batch_loss 4.445444583892822!\n",
      "8000/11838 Training....batch_loss 4.513283729553223!\n",
      "9000/11838 Training....batch_loss 4.429348945617676!\n",
      "10000/11838 Training....batch_loss 4.535686016082764!\n",
      "11000/11838 Training....batch_loss 4.416092395782471!\n",
      "Epoch 6/10 loss 4.340423107147217\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.7293033599853516!\n",
      "2000/11838 Training....batch_loss 4.3517231941223145!\n",
      "3000/11838 Training....batch_loss 4.131515979766846!\n",
      "4000/11838 Training....batch_loss 3.8911843299865723!\n",
      "5000/11838 Training....batch_loss 4.080914497375488!\n",
      "6000/11838 Training....batch_loss 4.049251556396484!\n",
      "7000/11838 Training....batch_loss 4.197037220001221!\n",
      "8000/11838 Training....batch_loss 4.251408576965332!\n",
      "9000/11838 Training....batch_loss 4.165958404541016!\n",
      "10000/11838 Training....batch_loss 4.259329795837402!\n",
      "11000/11838 Training....batch_loss 4.147377967834473!\n",
      "Epoch 7/10 loss 4.077239990234375\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.490851402282715!\n",
      "2000/11838 Training....batch_loss 4.093757152557373!\n",
      "3000/11838 Training....batch_loss 3.8891069889068604!\n",
      "4000/11838 Training....batch_loss 3.663530111312866!\n",
      "5000/11838 Training....batch_loss 3.8485662937164307!\n",
      "6000/11838 Training....batch_loss 3.8255670070648193!\n",
      "7000/11838 Training....batch_loss 3.976043462753296!\n",
      "8000/11838 Training....batch_loss 4.034379482269287!\n",
      "9000/11838 Training....batch_loss 3.957285165786743!\n",
      "10000/11838 Training....batch_loss 4.052628040313721!\n",
      "11000/11838 Training....batch_loss 3.949934244155884!\n",
      "Epoch 8/10 loss 3.8870503902435303\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.3636085987091064!\n",
      "2000/11838 Training....batch_loss 3.956638813018799!\n",
      "3000/11838 Training....batch_loss 3.7602474689483643!\n",
      "4000/11838 Training....batch_loss 3.5436127185821533!\n",
      "5000/11838 Training....batch_loss 3.726278781890869!\n",
      "6000/11838 Training....batch_loss 3.7084219455718994!\n",
      "7000/11838 Training....batch_loss 3.8612253665924072!\n",
      "8000/11838 Training....batch_loss 3.921419143676758!\n",
      "9000/11838 Training....batch_loss 3.848942279815674!\n",
      "10000/11838 Training....batch_loss 3.9456064701080322!\n",
      "11000/11838 Training....batch_loss 3.847843647003174!\n",
      "Epoch 9/10 loss 3.7888636589050293\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.3054709434509277!\n",
      "2000/11838 Training....batch_loss 3.8921396732330322!\n",
      "3000/11838 Training....batch_loss 3.6988635063171387!\n",
      "4000/11838 Training....batch_loss 3.4862558841705322!\n",
      "5000/11838 Training....batch_loss 3.6651291847229004!\n",
      "6000/11838 Training....batch_loss 3.649364471435547!\n",
      "7000/11838 Training....batch_loss 3.803157091140747!\n",
      "8000/11838 Training....batch_loss 3.863945484161377!\n",
      "9000/11838 Training....batch_loss 3.7934417724609375!\n",
      "10000/11838 Training....batch_loss 3.889927625656128!\n",
      "11000/11838 Training....batch_loss 3.7944040298461914!\n",
      "Epoch 10/10 loss 3.7371907234191895\n",
      "LAST SAVE MODEL EPOCH 10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!python test.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(512, 512, 2)\n",
      "(512, 512, 2)\n",
      "luma_recon : 37.93458191924171 / 0.959779596949771 luma_bicubic : 37.12439095115347 / 0.9520453248230841\n",
      "Saved [test/img_001_org.png]\n",
      "Saved [test/img_001_recon.png]\n",
      "Saved [test/img_001_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(288, 288, 2)\n",
      "(288, 288, 2)\n",
      "luma_recon : 38.703992300848554 / 0.979873922264568 luma_bicubic : 36.73963134778147 / 0.9719822655200628\n",
      "Saved [test/img_002_org.png]\n",
      "Saved [test/img_002_recon.png]\n",
      "Saved [test/img_002_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(256, 256, 2)\n",
      "(256, 256, 2)\n",
      "luma_recon : 29.00976105590511 / 0.9341022274690308 luma_bicubic : 27.485831267157707 / 0.9150048104031713\n",
      "Saved [test/img_003_org.png]\n",
      "Saved [test/img_003_recon.png]\n",
      "Saved [test/img_003_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(280, 280, 2)\n",
      "(280, 280, 2)\n",
      "luma_recon : 35.378096163312826 / 0.8748006478918012 luma_bicubic : 34.90877421068931 / 0.8621171629666867\n",
      "Saved [test/img_004_org.png]\n",
      "Saved [test/img_004_recon.png]\n",
      "Saved [test/img_004_bi.png]\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(344, 228, 2)\n",
      "(344, 228, 2)\n",
      "luma_recon : 33.52038203368568 / 0.957321278320769 luma_bicubic : 32.1838099118808 / 0.9474995647197535\n",
      "Saved [test/img_005_org.png]\n",
      "Saved [test/img_005_recon.png]\n",
      "Saved [test/img_005_bi.png]\n",
      "avg_psnr / avg_ssim : 34.90936269459878 / 0.941175534579188\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 6\n",
    "\n",
    "* Train_Data: ( bsd200 + yang91 ) * 8 (augmentation)\n",
    "* Image_batch_size(Low Resolution size) = 32\n",
    "* Batch_size = 64\n",
    "* Scale_factor = 2\n",
    "* Learning rate = 0.00002, epochs = 50\n",
    "* Loss : MSEloss, Optimizer : Adam"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "!python train.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "NUM_CPU :  12\n",
      "Augmentation Target :  data/bsd200\n",
      "Augmentation Target :  data/yang91\n",
      "291  LIST UP COMPLETED!!\n",
      "augmentation done..!!\n",
      "Next Level : Split images Using Window, 2328 pic\n",
      "split progressing : 50 1164/2328\n",
      "split progressing : 100 2328/2328\n",
      "DCSCN(\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1301, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1301, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (upconv): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixelshufflerlayer): PixelShuffle(upscale_factor=2)\n",
      "  (reconv): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)\n",
      ")\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 32325488640.0!\n",
      "2000/11838 Training....batch_loss 16934833152.0!\n",
      "3000/11838 Training....batch_loss 11475551232.0!\n",
      "4000/11838 Training....batch_loss 8664271872.0!\n",
      "5000/11838 Training....batch_loss 6953121792.0!\n",
      "6000/11838 Training....batch_loss 5805321728.0!\n",
      "7000/11838 Training....batch_loss 4980983296.0!\n",
      "8000/11838 Training....batch_loss 4360822784.0!\n",
      "9000/11838 Training....batch_loss 3877707776.0!\n",
      "10000/11838 Training....batch_loss 3490838272.0!\n",
      "11000/11838 Training....batch_loss 3174070528.0!\n",
      "Epoch 1/50 loss 2949597696.0\n",
      "SAVE MODEL EPOCH 0\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 1911185.625!\n",
      "2000/11838 Training....batch_loss 1419435.375!\n",
      "3000/11838 Training....batch_loss 1120521.25!\n",
      "4000/11838 Training....batch_loss 907705.5625!\n",
      "5000/11838 Training....batch_loss 752705.625!\n",
      "6000/11838 Training....batch_loss 639822.4375!\n",
      "7000/11838 Training....batch_loss 553338.9375!\n",
      "8000/11838 Training....batch_loss 486538.0!\n",
      "9000/11838 Training....batch_loss 433851.4375!\n",
      "10000/11838 Training....batch_loss 391390.46875!\n",
      "11000/11838 Training....batch_loss 356491.6875!\n",
      "Epoch 2/50 loss 331529.03125\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3148.09716796875!\n",
      "2000/11838 Training....batch_loss 2456.56103515625!\n",
      "3000/11838 Training....batch_loss 2096.872802734375!\n",
      "4000/11838 Training....batch_loss 1800.64111328125!\n",
      "5000/11838 Training....batch_loss 1641.7550048828125!\n",
      "6000/11838 Training....batch_loss 1453.6802978515625!\n",
      "7000/11838 Training....batch_loss 1303.1614990234375!\n",
      "8000/11838 Training....batch_loss 1163.345458984375!\n",
      "9000/11838 Training....batch_loss 1084.445068359375!\n",
      "10000/11838 Training....batch_loss 984.2055053710938!\n",
      "11000/11838 Training....batch_loss 901.5477905273438!\n",
      "Epoch 3/50 loss 840.9169921875\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 42.9680290222168!\n",
      "2000/11838 Training....batch_loss 34.343685150146484!\n",
      "3000/11838 Training....batch_loss 29.37495231628418!\n",
      "4000/11838 Training....batch_loss 27.29281234741211!\n",
      "5000/11838 Training....batch_loss 23.9641170501709!\n",
      "6000/11838 Training....batch_loss 21.56808090209961!\n",
      "7000/11838 Training....batch_loss 19.72873878479004!\n",
      "8000/11838 Training....batch_loss 18.119003295898438!\n",
      "9000/11838 Training....batch_loss 16.706300735473633!\n",
      "10000/11838 Training....batch_loss 15.696393013000488!\n",
      "11000/11838 Training....batch_loss 14.694012641906738!\n",
      "Epoch 4/50 loss 13.920011520385742\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 4.332573890686035!\n",
      "2000/11838 Training....batch_loss 4.962339878082275!\n",
      "3000/11838 Training....batch_loss 4.674690246582031!\n",
      "4000/11838 Training....batch_loss 4.364255428314209!\n",
      "5000/11838 Training....batch_loss 4.546346187591553!\n",
      "6000/11838 Training....batch_loss 4.4849534034729!\n",
      "7000/11838 Training....batch_loss 4.633846282958984!\n",
      "8000/11838 Training....batch_loss 4.681940078735352!\n",
      "9000/11838 Training....batch_loss 4.5820536613464355!\n",
      "10000/11838 Training....batch_loss 4.681988716125488!\n",
      "11000/11838 Training....batch_loss 4.552530765533447!\n",
      "Epoch 5/50 loss 4.470175743103027\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.7468059062957764!\n",
      "2000/11838 Training....batch_loss 4.353150367736816!\n",
      "3000/11838 Training....batch_loss 4.119797706604004!\n",
      "4000/11838 Training....batch_loss 3.8691763877868652!\n",
      "5000/11838 Training....batch_loss 4.071795463562012!\n",
      "6000/11838 Training....batch_loss 4.030592918395996!\n",
      "7000/11838 Training....batch_loss 4.166585922241211!\n",
      "8000/11838 Training....batch_loss 4.212744235992432!\n",
      "9000/11838 Training....batch_loss 4.122109889984131!\n",
      "10000/11838 Training....batch_loss 4.20838737487793!\n",
      "11000/11838 Training....batch_loss 4.094316005706787!\n",
      "Epoch 6/50 loss 4.023004531860352\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.3848233222961426!\n",
      "2000/11838 Training....batch_loss 3.9782326221466064!\n",
      "3000/11838 Training....batch_loss 3.7775511741638184!\n",
      "4000/11838 Training....batch_loss 3.557511568069458!\n",
      "5000/11838 Training....batch_loss 3.755805492401123!\n",
      "6000/11838 Training....batch_loss 3.7332522869110107!\n",
      "7000/11838 Training....batch_loss 3.8812215328216553!\n",
      "8000/11838 Training....batch_loss 3.9382829666137695!\n",
      "9000/11838 Training....batch_loss 3.8628389835357666!\n",
      "10000/11838 Training....batch_loss 3.956961154937744!\n",
      "11000/11838 Training....batch_loss 3.857344627380371!\n",
      "Epoch 7/50 loss 3.797053813934326\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.288146734237671!\n",
      "2000/11838 Training....batch_loss 3.87308406829834!\n",
      "3000/11838 Training....batch_loss 3.679374933242798!\n",
      "4000/11838 Training....batch_loss 3.4665234088897705!\n",
      "5000/11838 Training....batch_loss 3.6525018215179443!\n",
      "6000/11838 Training....batch_loss 3.6341795921325684!\n",
      "7000/11838 Training....batch_loss 3.785417318344116!\n",
      "8000/11838 Training....batch_loss 3.84393310546875!\n",
      "9000/11838 Training....batch_loss 3.7715721130371094!\n",
      "10000/11838 Training....batch_loss 3.865680456161499!\n",
      "11000/11838 Training....batch_loss 3.769650936126709!\n",
      "Epoch 8/50 loss 3.7122068405151367\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.2354209423065186!\n",
      "2000/11838 Training....batch_loss 3.8118810653686523!\n",
      "3000/11838 Training....batch_loss 3.619497776031494!\n",
      "4000/11838 Training....batch_loss 3.4091663360595703!\n",
      "5000/11838 Training....batch_loss 3.583009719848633!\n",
      "6000/11838 Training....batch_loss 3.565484046936035!\n",
      "7000/11838 Training....batch_loss 3.7185726165771484!\n",
      "8000/11838 Training....batch_loss 3.7770824432373047!\n",
      "9000/11838 Training....batch_loss 3.7055704593658447!\n",
      "10000/11838 Training....batch_loss 3.7989814281463623!\n",
      "11000/11838 Training....batch_loss 3.705157518386841!\n",
      "Epoch 9/50 loss 3.6495089530944824\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.193218946456909!\n",
      "2000/11838 Training....batch_loss 3.7644875049591064!\n",
      "3000/11838 Training....batch_loss 3.573300838470459!\n",
      "4000/11838 Training....batch_loss 3.3647401332855225!\n",
      "5000/11838 Training....batch_loss 3.5289151668548584!\n",
      "6000/11838 Training....batch_loss 3.5129621028900146!\n",
      "7000/11838 Training....batch_loss 3.668261766433716!\n",
      "8000/11838 Training....batch_loss 3.7271854877471924!\n",
      "9000/11838 Training....batch_loss 3.656745195388794!\n",
      "10000/11838 Training....batch_loss 3.7500882148742676!\n",
      "11000/11838 Training....batch_loss 3.6580734252929688!\n",
      "Epoch 10/50 loss 3.603799343109131\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.165980100631714!\n",
      "2000/11838 Training....batch_loss 3.730250358581543!\n",
      "3000/11838 Training....batch_loss 3.539842128753662!\n",
      "4000/11838 Training....batch_loss 3.3324668407440186!\n",
      "5000/11838 Training....batch_loss 3.489150285720825!\n",
      "6000/11838 Training....batch_loss 3.4742555618286133!\n",
      "7000/11838 Training....batch_loss 3.631282091140747!\n",
      "8000/11838 Training....batch_loss 3.6903464794158936!\n",
      "9000/11838 Training....batch_loss 3.6206295490264893!\n",
      "10000/11838 Training....batch_loss 3.713688611984253!\n",
      "11000/11838 Training....batch_loss 3.6227524280548096!\n",
      "Epoch 11/50 loss 3.569397449493408\n",
      "SAVE MODEL EPOCH 10\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.142866849899292!\n",
      "2000/11838 Training....batch_loss 3.703184127807617!\n",
      "3000/11838 Training....batch_loss 3.513624668121338!\n",
      "4000/11838 Training....batch_loss 3.3069119453430176!\n",
      "5000/11838 Training....batch_loss 3.457641363143921!\n",
      "6000/11838 Training....batch_loss 3.443835496902466!\n",
      "7000/11838 Training....batch_loss 3.6014840602874756!\n",
      "8000/11838 Training....batch_loss 3.6605193614959717!\n",
      "9000/11838 Training....batch_loss 3.5910708904266357!\n",
      "10000/11838 Training....batch_loss 3.6837737560272217!\n",
      "11000/11838 Training....batch_loss 3.5937039852142334!\n",
      "Epoch 12/50 loss 3.541017532348633\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.1238393783569336!\n",
      "2000/11838 Training....batch_loss 3.679800271987915!\n",
      "3000/11838 Training....batch_loss 3.4902663230895996!\n",
      "4000/11838 Training....batch_loss 3.2841732501983643!\n",
      "5000/11838 Training....batch_loss 3.4296767711639404!\n",
      "6000/11838 Training....batch_loss 3.4165165424346924!\n",
      "7000/11838 Training....batch_loss 3.5752902030944824!\n",
      "8000/11838 Training....batch_loss 3.634484052658081!\n",
      "9000/11838 Training....batch_loss 3.565399646759033!\n",
      "10000/11838 Training....batch_loss 3.657968282699585!\n",
      "11000/11838 Training....batch_loss 3.56872820854187!\n",
      "Epoch 13/50 loss 3.5165493488311768\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.107330799102783!\n",
      "2000/11838 Training....batch_loss 3.6591219902038574!\n",
      "3000/11838 Training....batch_loss 3.470517873764038!\n",
      "4000/11838 Training....batch_loss 3.2647998332977295!\n",
      "5000/11838 Training....batch_loss 3.405928134918213!\n",
      "6000/11838 Training....batch_loss 3.3933281898498535!\n",
      "7000/11838 Training....batch_loss 3.552980899810791!\n",
      "8000/11838 Training....batch_loss 3.612008810043335!\n",
      "9000/11838 Training....batch_loss 3.5431923866271973!\n",
      "10000/11838 Training....batch_loss 3.635626792907715!\n",
      "11000/11838 Training....batch_loss 3.5469326972961426!\n",
      "Epoch 14/50 loss 3.495220899581909\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.092712640762329!\n",
      "2000/11838 Training....batch_loss 3.640897750854492!\n",
      "3000/11838 Training....batch_loss 3.4523491859436035!\n",
      "4000/11838 Training....batch_loss 3.246993064880371!\n",
      "5000/11838 Training....batch_loss 3.384565591812134!\n",
      "6000/11838 Training....batch_loss 3.372579336166382!\n",
      "7000/11838 Training....batch_loss 3.5331263542175293!\n",
      "8000/11838 Training....batch_loss 3.5919973850250244!\n",
      "9000/11838 Training....batch_loss 3.523435115814209!\n",
      "10000/11838 Training....batch_loss 3.6157894134521484!\n",
      "11000/11838 Training....batch_loss 3.527695655822754!\n",
      "Epoch 15/50 loss 3.4762372970581055\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.0800507068634033!\n",
      "2000/11838 Training....batch_loss 3.6250879764556885!\n",
      "3000/11838 Training....batch_loss 3.4366838932037354!\n",
      "4000/11838 Training....batch_loss 3.231243133544922!\n",
      "5000/11838 Training....batch_loss 3.3662827014923096!\n",
      "6000/11838 Training....batch_loss 3.3548507690429688!\n",
      "7000/11838 Training....batch_loss 3.516056776046753!\n",
      "8000/11838 Training....batch_loss 3.57474684715271!\n",
      "9000/11838 Training....batch_loss 3.506291389465332!\n",
      "10000/11838 Training....batch_loss 3.5984046459198!\n",
      "11000/11838 Training....batch_loss 3.5107367038726807!\n",
      "Epoch 16/50 loss 3.4596192836761475\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.0679869651794434!\n",
      "2000/11838 Training....batch_loss 3.6096842288970947!\n",
      "3000/11838 Training....batch_loss 3.421800136566162!\n",
      "4000/11838 Training....batch_loss 3.2169814109802246!\n",
      "5000/11838 Training....batch_loss 3.349553108215332!\n",
      "6000/11838 Training....batch_loss 3.3382179737091064!\n",
      "7000/11838 Training....batch_loss 3.4998385906219482!\n",
      "8000/11838 Training....batch_loss 3.55845046043396!\n",
      "9000/11838 Training....batch_loss 3.4901933670043945!\n",
      "10000/11838 Training....batch_loss 3.5822346210479736!\n",
      "11000/11838 Training....batch_loss 3.495086193084717!\n",
      "Epoch 17/50 loss 3.444204568862915\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.057835817337036!\n",
      "2000/11838 Training....batch_loss 3.5963070392608643!\n",
      "3000/11838 Training....batch_loss 3.4087138175964355!\n",
      "4000/11838 Training....batch_loss 3.203969955444336!\n",
      "5000/11838 Training....batch_loss 3.334334373474121!\n",
      "6000/11838 Training....batch_loss 3.323235273361206!\n",
      "7000/11838 Training....batch_loss 3.485304832458496!\n",
      "8000/11838 Training....batch_loss 3.5439813137054443!\n",
      "9000/11838 Training....batch_loss 3.476020574569702!\n",
      "10000/11838 Training....batch_loss 3.5679433345794678!\n",
      "11000/11838 Training....batch_loss 3.4812839031219482!\n",
      "Epoch 18/50 loss 3.430706262588501\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.047471523284912!\n",
      "2000/11838 Training....batch_loss 3.5842278003692627!\n",
      "3000/11838 Training....batch_loss 3.397019147872925!\n",
      "4000/11838 Training....batch_loss 3.192403793334961!\n",
      "5000/11838 Training....batch_loss 3.321279525756836!\n",
      "6000/11838 Training....batch_loss 3.3103554248809814!\n",
      "7000/11838 Training....batch_loss 3.4729037284851074!\n",
      "8000/11838 Training....batch_loss 3.531550168991089!\n",
      "9000/11838 Training....batch_loss 3.463775634765625!\n",
      "10000/11838 Training....batch_loss 3.555412769317627!\n",
      "11000/11838 Training....batch_loss 3.4691483974456787!\n",
      "Epoch 19/50 loss 3.4187700748443604\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.0383806228637695!\n",
      "2000/11838 Training....batch_loss 3.573058843612671!\n",
      "3000/11838 Training....batch_loss 3.3863515853881836!\n",
      "4000/11838 Training....batch_loss 3.1820664405822754!\n",
      "5000/11838 Training....batch_loss 3.3098647594451904!\n",
      "6000/11838 Training....batch_loss 3.299020528793335!\n",
      "7000/11838 Training....batch_loss 3.461766481399536!\n",
      "8000/11838 Training....batch_loss 3.520254611968994!\n",
      "9000/11838 Training....batch_loss 3.4526865482330322!\n",
      "10000/11838 Training....batch_loss 3.544229507446289!\n",
      "11000/11838 Training....batch_loss 3.4582769870758057!\n",
      "Epoch 20/50 loss 3.408208131790161\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.0318100452423096!\n",
      "2000/11838 Training....batch_loss 3.563930034637451!\n",
      "3000/11838 Training....batch_loss 3.377406120300293!\n",
      "4000/11838 Training....batch_loss 3.1731324195861816!\n",
      "5000/11838 Training....batch_loss 3.2999322414398193!\n",
      "6000/11838 Training....batch_loss 3.2890946865081787!\n",
      "7000/11838 Training....batch_loss 3.452056884765625!\n",
      "8000/11838 Training....batch_loss 3.510580539703369!\n",
      "9000/11838 Training....batch_loss 3.4430816173553467!\n",
      "10000/11838 Training....batch_loss 3.534519910812378!\n",
      "11000/11838 Training....batch_loss 3.448835849761963!\n",
      "Epoch 21/50 loss 3.3989038467407227\n",
      "SAVE MODEL EPOCH 20\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.0261754989624023!\n",
      "2000/11838 Training....batch_loss 3.557086706161499!\n",
      "3000/11838 Training....batch_loss 3.3702962398529053!\n",
      "4000/11838 Training....batch_loss 3.165969133377075!\n",
      "5000/11838 Training....batch_loss 3.291308879852295!\n",
      "6000/11838 Training....batch_loss 3.2804319858551025!\n",
      "7000/11838 Training....batch_loss 3.4437103271484375!\n",
      "8000/11838 Training....batch_loss 3.5022921562194824!\n",
      "9000/11838 Training....batch_loss 3.434919834136963!\n",
      "10000/11838 Training....batch_loss 3.526242971420288!\n",
      "11000/11838 Training....batch_loss 3.440748929977417!\n",
      "Epoch 22/50 loss 3.3909051418304443\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.01969575881958!\n",
      "2000/11838 Training....batch_loss 3.5488290786743164!\n",
      "3000/11838 Training....batch_loss 3.3623502254486084!\n",
      "4000/11838 Training....batch_loss 3.1582248210906982!\n",
      "5000/11838 Training....batch_loss 3.2826642990112305!\n",
      "6000/11838 Training....batch_loss 3.2721145153045654!\n",
      "7000/11838 Training....batch_loss 3.4355902671813965!\n",
      "8000/11838 Training....batch_loss 3.49406361579895!\n",
      "9000/11838 Training....batch_loss 3.4268596172332764!\n",
      "10000/11838 Training....batch_loss 3.51802659034729!\n",
      "11000/11838 Training....batch_loss 3.432844638824463!\n",
      "Epoch 23/50 loss 3.3831374645233154\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.014617919921875!\n",
      "2000/11838 Training....batch_loss 3.542186737060547!\n",
      "3000/11838 Training....batch_loss 3.3558883666992188!\n",
      "4000/11838 Training....batch_loss 3.152048349380493!\n",
      "5000/11838 Training....batch_loss 3.275517702102661!\n",
      "6000/11838 Training....batch_loss 3.2649993896484375!\n",
      "7000/11838 Training....batch_loss 3.4286069869995117!\n",
      "8000/11838 Training....batch_loss 3.487067461013794!\n",
      "9000/11838 Training....batch_loss 3.419961452484131!\n",
      "10000/11838 Training....batch_loss 3.511056900024414!\n",
      "11000/11838 Training....batch_loss 3.426201343536377!\n",
      "Epoch 24/50 loss 3.376615047454834\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.0096726417541504!\n",
      "2000/11838 Training....batch_loss 3.535850763320923!\n",
      "3000/11838 Training....batch_loss 3.349741220474243!\n",
      "4000/11838 Training....batch_loss 3.145961284637451!\n",
      "5000/11838 Training....batch_loss 3.268904685974121!\n",
      "6000/11838 Training....batch_loss 3.2584245204925537!\n",
      "7000/11838 Training....batch_loss 3.4221885204315186!\n",
      "8000/11838 Training....batch_loss 3.480720281600952!\n",
      "9000/11838 Training....batch_loss 3.4137096405029297!\n",
      "10000/11838 Training....batch_loss 3.504676580429077!\n",
      "11000/11838 Training....batch_loss 3.4199795722961426!\n",
      "Epoch 25/50 loss 3.370453357696533\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.004816770553589!\n",
      "2000/11838 Training....batch_loss 3.5309650897979736!\n",
      "3000/11838 Training....batch_loss 3.344538688659668!\n",
      "4000/11838 Training....batch_loss 3.140681266784668!\n",
      "5000/11838 Training....batch_loss 3.262695550918579!\n",
      "6000/11838 Training....batch_loss 3.252379894256592!\n",
      "7000/11838 Training....batch_loss 3.416231632232666!\n",
      "8000/11838 Training....batch_loss 3.474515438079834!\n",
      "9000/11838 Training....batch_loss 3.4076836109161377!\n",
      "10000/11838 Training....batch_loss 3.4987189769744873!\n",
      "11000/11838 Training....batch_loss 3.4141106605529785!\n",
      "Epoch 26/50 loss 3.3648388385772705\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 3.0012876987457275!\n",
      "2000/11838 Training....batch_loss 3.5252773761749268!\n",
      "3000/11838 Training....batch_loss 3.339028835296631!\n",
      "4000/11838 Training....batch_loss 3.1350173950195312!\n",
      "5000/11838 Training....batch_loss 3.2566757202148438!\n",
      "6000/11838 Training....batch_loss 3.2464706897735596!\n",
      "7000/11838 Training....batch_loss 3.4105403423309326!\n",
      "8000/11838 Training....batch_loss 3.468996047973633!\n",
      "9000/11838 Training....batch_loss 3.402226448059082!\n",
      "10000/11838 Training....batch_loss 3.492964744567871!\n",
      "11000/11838 Training....batch_loss 3.4085748195648193!\n",
      "Epoch 27/50 loss 3.3593859672546387\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9971742630004883!\n",
      "2000/11838 Training....batch_loss 3.5197298526763916!\n",
      "3000/11838 Training....batch_loss 3.3340516090393066!\n",
      "4000/11838 Training....batch_loss 3.130667209625244!\n",
      "5000/11838 Training....batch_loss 3.2520370483398438!\n",
      "6000/11838 Training....batch_loss 3.2415952682495117!\n",
      "7000/11838 Training....batch_loss 3.405622959136963!\n",
      "8000/11838 Training....batch_loss 3.46401309967041!\n",
      "9000/11838 Training....batch_loss 3.39735746383667!\n",
      "10000/11838 Training....batch_loss 3.4881558418273926!\n",
      "11000/11838 Training....batch_loss 3.4039504528045654!\n",
      "Epoch 28/50 loss 3.354846715927124\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9947657585144043!\n",
      "2000/11838 Training....batch_loss 3.516481637954712!\n",
      "3000/11838 Training....batch_loss 3.330395460128784!\n",
      "4000/11838 Training....batch_loss 3.1269681453704834!\n",
      "5000/11838 Training....batch_loss 3.2476024627685547!\n",
      "6000/11838 Training....batch_loss 3.2372992038726807!\n",
      "7000/11838 Training....batch_loss 3.4016799926757812!\n",
      "8000/11838 Training....batch_loss 3.460094451904297!\n",
      "9000/11838 Training....batch_loss 3.3934547901153564!\n",
      "10000/11838 Training....batch_loss 3.484133005142212!\n",
      "11000/11838 Training....batch_loss 3.400024175643921!\n",
      "Epoch 29/50 loss 3.3509910106658936\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9918243885040283!\n",
      "2000/11838 Training....batch_loss 3.5123698711395264!\n",
      "3000/11838 Training....batch_loss 3.3265762329101562!\n",
      "4000/11838 Training....batch_loss 3.1229515075683594!\n",
      "5000/11838 Training....batch_loss 3.2434232234954834!\n",
      "6000/11838 Training....batch_loss 3.2331933975219727!\n",
      "7000/11838 Training....batch_loss 3.3973283767700195!\n",
      "8000/11838 Training....batch_loss 3.4555983543395996!\n",
      "9000/11838 Training....batch_loss 3.3891546726226807!\n",
      "10000/11838 Training....batch_loss 3.479794502258301!\n",
      "11000/11838 Training....batch_loss 3.395873785018921!\n",
      "Epoch 30/50 loss 3.346935987472534\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.988413095474243!\n",
      "2000/11838 Training....batch_loss 3.5082621574401855!\n",
      "3000/11838 Training....batch_loss 3.3227920532226562!\n",
      "4000/11838 Training....batch_loss 3.119622230529785!\n",
      "5000/11838 Training....batch_loss 3.2395811080932617!\n",
      "6000/11838 Training....batch_loss 3.229525327682495!\n",
      "7000/11838 Training....batch_loss 3.3937742710113525!\n",
      "8000/11838 Training....batch_loss 3.4521055221557617!\n",
      "9000/11838 Training....batch_loss 3.3856770992279053!\n",
      "10000/11838 Training....batch_loss 3.4761335849761963!\n",
      "11000/11838 Training....batch_loss 3.392298936843872!\n",
      "Epoch 31/50 loss 3.3434321880340576\n",
      "SAVE MODEL EPOCH 30\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9863240718841553!\n",
      "2000/11838 Training....batch_loss 3.505784273147583!\n",
      "3000/11838 Training....batch_loss 3.3199496269226074!\n",
      "4000/11838 Training....batch_loss 3.1164097785949707!\n",
      "5000/11838 Training....batch_loss 3.2358226776123047!\n",
      "6000/11838 Training....batch_loss 3.2258336544036865!\n",
      "7000/11838 Training....batch_loss 3.390198230743408!\n",
      "8000/11838 Training....batch_loss 3.4485902786254883!\n",
      "9000/11838 Training....batch_loss 3.382136106491089!\n",
      "10000/11838 Training....batch_loss 3.4726343154907227!\n",
      "11000/11838 Training....batch_loss 3.38883900642395!\n",
      "Epoch 32/50 loss 3.340014696121216\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9833567142486572!\n",
      "2000/11838 Training....batch_loss 3.502007246017456!\n",
      "3000/11838 Training....batch_loss 3.316485643386841!\n",
      "4000/11838 Training....batch_loss 3.113175868988037!\n",
      "5000/11838 Training....batch_loss 3.232147455215454!\n",
      "6000/11838 Training....batch_loss 3.2220869064331055!\n",
      "7000/11838 Training....batch_loss 3.3865623474121094!\n",
      "8000/11838 Training....batch_loss 3.444932460784912!\n",
      "9000/11838 Training....batch_loss 3.378577709197998!\n",
      "10000/11838 Training....batch_loss 3.4690804481506348!\n",
      "11000/11838 Training....batch_loss 3.38543963432312!\n",
      "Epoch 33/50 loss 3.3367130756378174\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9812893867492676!\n",
      "2000/11838 Training....batch_loss 3.499845027923584!\n",
      "3000/11838 Training....batch_loss 3.313913345336914!\n",
      "4000/11838 Training....batch_loss 3.110525608062744!\n",
      "5000/11838 Training....batch_loss 3.2290589809417725!\n",
      "6000/11838 Training....batch_loss 3.2190780639648438!\n",
      "7000/11838 Training....batch_loss 3.3834691047668457!\n",
      "8000/11838 Training....batch_loss 3.4417686462402344!\n",
      "9000/11838 Training....batch_loss 3.375535011291504!\n",
      "10000/11838 Training....batch_loss 3.465792179107666!\n",
      "11000/11838 Training....batch_loss 3.3822176456451416!\n",
      "Epoch 34/50 loss 3.333580255508423\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9798524379730225!\n",
      "2000/11838 Training....batch_loss 3.4971604347229004!\n",
      "3000/11838 Training....batch_loss 3.311305046081543!\n",
      "4000/11838 Training....batch_loss 3.1078696250915527!\n",
      "5000/11838 Training....batch_loss 3.226078748703003!\n",
      "6000/11838 Training....batch_loss 3.21596622467041!\n",
      "7000/11838 Training....batch_loss 3.380317449569702!\n",
      "8000/11838 Training....batch_loss 3.438472032546997!\n",
      "9000/11838 Training....batch_loss 3.372298002243042!\n",
      "10000/11838 Training....batch_loss 3.462630271911621!\n",
      "11000/11838 Training....batch_loss 3.3792519569396973!\n",
      "Epoch 35/50 loss 3.33073091506958\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.977146625518799!\n",
      "2000/11838 Training....batch_loss 3.494710922241211!\n",
      "3000/11838 Training....batch_loss 3.3089118003845215!\n",
      "4000/11838 Training....batch_loss 3.10593581199646!\n",
      "5000/11838 Training....batch_loss 3.2239246368408203!\n",
      "6000/11838 Training....batch_loss 3.213712215423584!\n",
      "7000/11838 Training....batch_loss 3.378243923187256!\n",
      "8000/11838 Training....batch_loss 3.4362189769744873!\n",
      "9000/11838 Training....batch_loss 3.369987726211548!\n",
      "10000/11838 Training....batch_loss 3.4601619243621826!\n",
      "11000/11838 Training....batch_loss 3.376784324645996!\n",
      "Epoch 36/50 loss 3.328277826309204\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.975097417831421!\n",
      "2000/11838 Training....batch_loss 3.491502285003662!\n",
      "3000/11838 Training....batch_loss 3.3064420223236084!\n",
      "4000/11838 Training....batch_loss 3.1032261848449707!\n",
      "5000/11838 Training....batch_loss 3.220843553543091!\n",
      "6000/11838 Training....batch_loss 3.2107770442962646!\n",
      "7000/11838 Training....batch_loss 3.3754117488861084!\n",
      "8000/11838 Training....batch_loss 3.4336602687835693!\n",
      "9000/11838 Training....batch_loss 3.36763072013855!\n",
      "10000/11838 Training....batch_loss 3.4578750133514404!\n",
      "11000/11838 Training....batch_loss 3.374551296234131!\n",
      "Epoch 37/50 loss 3.326033592224121\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9740793704986572!\n",
      "2000/11838 Training....batch_loss 3.4899532794952393!\n",
      "3000/11838 Training....batch_loss 3.304218292236328!\n",
      "4000/11838 Training....batch_loss 3.1007509231567383!\n",
      "5000/11838 Training....batch_loss 3.218482255935669!\n",
      "6000/11838 Training....batch_loss 3.2087466716766357!\n",
      "7000/11838 Training....batch_loss 3.373450756072998!\n",
      "8000/11838 Training....batch_loss 3.431685209274292!\n",
      "9000/11838 Training....batch_loss 3.365537643432617!\n",
      "10000/11838 Training....batch_loss 3.4558322429656982!\n",
      "11000/11838 Training....batch_loss 3.3725266456604004!\n",
      "Epoch 38/50 loss 3.324021816253662\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.973043918609619!\n",
      "2000/11838 Training....batch_loss 3.4882214069366455!\n",
      "3000/11838 Training....batch_loss 3.302537441253662!\n",
      "4000/11838 Training....batch_loss 3.0992605686187744!\n",
      "5000/11838 Training....batch_loss 3.216500759124756!\n",
      "6000/11838 Training....batch_loss 3.206435203552246!\n",
      "7000/11838 Training....batch_loss 3.3710250854492188!\n",
      "8000/11838 Training....batch_loss 3.429266929626465!\n",
      "9000/11838 Training....batch_loss 3.363182306289673!\n",
      "10000/11838 Training....batch_loss 3.4531261920928955!\n",
      "11000/11838 Training....batch_loss 3.3700294494628906!\n",
      "Epoch 39/50 loss 3.3216710090637207\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9705147743225098!\n",
      "2000/11838 Training....batch_loss 3.4858570098876953!\n",
      "3000/11838 Training....batch_loss 3.3003275394439697!\n",
      "4000/11838 Training....batch_loss 3.097285509109497!\n",
      "5000/11838 Training....batch_loss 3.213984966278076!\n",
      "6000/11838 Training....batch_loss 3.2042031288146973!\n",
      "7000/11838 Training....batch_loss 3.368818521499634!\n",
      "8000/11838 Training....batch_loss 3.4270195960998535!\n",
      "9000/11838 Training....batch_loss 3.3609447479248047!\n",
      "10000/11838 Training....batch_loss 3.451068162918091!\n",
      "11000/11838 Training....batch_loss 3.368020534515381!\n",
      "Epoch 40/50 loss 3.3197460174560547\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.969799518585205!\n",
      "2000/11838 Training....batch_loss 3.4844937324523926!\n",
      "3000/11838 Training....batch_loss 3.298832893371582!\n",
      "4000/11838 Training....batch_loss 3.0957019329071045!\n",
      "5000/11838 Training....batch_loss 3.212196111679077!\n",
      "6000/11838 Training....batch_loss 3.2023348808288574!\n",
      "7000/11838 Training....batch_loss 3.3669004440307617!\n",
      "8000/11838 Training....batch_loss 3.4252490997314453!\n",
      "9000/11838 Training....batch_loss 3.359358072280884!\n",
      "10000/11838 Training....batch_loss 3.4493958950042725!\n",
      "11000/11838 Training....batch_loss 3.36641263961792!\n",
      "Epoch 41/50 loss 3.3180887699127197\n",
      "SAVE MODEL EPOCH 40\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.968010902404785!\n",
      "2000/11838 Training....batch_loss 3.4822826385498047!\n",
      "3000/11838 Training....batch_loss 3.296985387802124!\n",
      "4000/11838 Training....batch_loss 3.0937345027923584!\n",
      "5000/11838 Training....batch_loss 3.2102582454681396!\n",
      "6000/11838 Training....batch_loss 3.2003371715545654!\n",
      "7000/11838 Training....batch_loss 3.3650996685028076!\n",
      "8000/11838 Training....batch_loss 3.423381805419922!\n",
      "9000/11838 Training....batch_loss 3.3574793338775635!\n",
      "10000/11838 Training....batch_loss 3.447514772415161!\n",
      "11000/11838 Training....batch_loss 3.3645737171173096!\n",
      "Epoch 42/50 loss 3.316281318664551\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.966428518295288!\n",
      "2000/11838 Training....batch_loss 3.480557680130005!\n",
      "3000/11838 Training....batch_loss 3.295295476913452!\n",
      "4000/11838 Training....batch_loss 3.0922060012817383!\n",
      "5000/11838 Training....batch_loss 3.208700180053711!\n",
      "6000/11838 Training....batch_loss 3.198843479156494!\n",
      "7000/11838 Training....batch_loss 3.363584041595459!\n",
      "8000/11838 Training....batch_loss 3.4216859340667725!\n",
      "9000/11838 Training....batch_loss 3.3558237552642822!\n",
      "10000/11838 Training....batch_loss 3.4458179473876953!\n",
      "11000/11838 Training....batch_loss 3.3629586696624756!\n",
      "Epoch 43/50 loss 3.314736843109131\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.964898109436035!\n",
      "2000/11838 Training....batch_loss 3.4792277812957764!\n",
      "3000/11838 Training....batch_loss 3.293727159500122!\n",
      "4000/11838 Training....batch_loss 3.0909600257873535!\n",
      "5000/11838 Training....batch_loss 3.2074050903320312!\n",
      "6000/11838 Training....batch_loss 3.197399139404297!\n",
      "7000/11838 Training....batch_loss 3.362041711807251!\n",
      "8000/11838 Training....batch_loss 3.4202022552490234!\n",
      "9000/11838 Training....batch_loss 3.354377269744873!\n",
      "10000/11838 Training....batch_loss 3.4443790912628174!\n",
      "11000/11838 Training....batch_loss 3.361555576324463!\n",
      "Epoch 44/50 loss 3.3133091926574707\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.965099573135376!\n",
      "2000/11838 Training....batch_loss 3.477515459060669!\n",
      "3000/11838 Training....batch_loss 3.292302131652832!\n",
      "4000/11838 Training....batch_loss 3.0892796516418457!\n",
      "5000/11838 Training....batch_loss 3.20511794090271!\n",
      "6000/11838 Training....batch_loss 3.195248603820801!\n",
      "7000/11838 Training....batch_loss 3.3601009845733643!\n",
      "8000/11838 Training....batch_loss 3.4183290004730225!\n",
      "9000/11838 Training....batch_loss 3.3525030612945557!\n",
      "10000/11838 Training....batch_loss 3.44240140914917!\n",
      "11000/11838 Training....batch_loss 3.3595829010009766!\n",
      "Epoch 45/50 loss 3.311405897140503\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.963364601135254!\n",
      "2000/11838 Training....batch_loss 3.476320505142212!\n",
      "3000/11838 Training....batch_loss 3.290919542312622!\n",
      "4000/11838 Training....batch_loss 3.0877902507781982!\n",
      "5000/11838 Training....batch_loss 3.2039082050323486!\n",
      "6000/11838 Training....batch_loss 3.19392991065979!\n",
      "7000/11838 Training....batch_loss 3.3586339950561523!\n",
      "8000/11838 Training....batch_loss 3.416860580444336!\n",
      "9000/11838 Training....batch_loss 3.350987672805786!\n",
      "10000/11838 Training....batch_loss 3.440891742706299!\n",
      "11000/11838 Training....batch_loss 3.3580994606018066!\n",
      "Epoch 46/50 loss 3.3099496364593506\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.962515115737915!\n",
      "2000/11838 Training....batch_loss 3.4747321605682373!\n",
      "3000/11838 Training....batch_loss 3.2896041870117188!\n",
      "4000/11838 Training....batch_loss 3.086698532104492!\n",
      "5000/11838 Training....batch_loss 3.202481269836426!\n",
      "6000/11838 Training....batch_loss 3.1925511360168457!\n",
      "7000/11838 Training....batch_loss 3.3572354316711426!\n",
      "8000/11838 Training....batch_loss 3.415496349334717!\n",
      "9000/11838 Training....batch_loss 3.3497159481048584!\n",
      "10000/11838 Training....batch_loss 3.439612865447998!\n",
      "11000/11838 Training....batch_loss 3.3568196296691895!\n",
      "Epoch 47/50 loss 3.308701276779175\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.960951328277588!\n",
      "2000/11838 Training....batch_loss 3.473649740219116!\n",
      "3000/11838 Training....batch_loss 3.2883596420288086!\n",
      "4000/11838 Training....batch_loss 3.085386037826538!\n",
      "5000/11838 Training....batch_loss 3.201192855834961!\n",
      "6000/11838 Training....batch_loss 3.1911873817443848!\n",
      "7000/11838 Training....batch_loss 3.3558671474456787!\n",
      "8000/11838 Training....batch_loss 3.413874626159668!\n",
      "9000/11838 Training....batch_loss 3.3481826782226562!\n",
      "10000/11838 Training....batch_loss 3.4382174015045166!\n",
      "11000/11838 Training....batch_loss 3.3555898666381836!\n",
      "Epoch 48/50 loss 3.307497501373291\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.961092948913574!\n",
      "2000/11838 Training....batch_loss 3.472468137741089!\n",
      "3000/11838 Training....batch_loss 3.287001848220825!\n",
      "4000/11838 Training....batch_loss 3.083826780319214!\n",
      "5000/11838 Training....batch_loss 3.1991727352142334!\n",
      "6000/11838 Training....batch_loss 3.18941330909729!\n",
      "7000/11838 Training....batch_loss 3.35434889793396!\n",
      "8000/11838 Training....batch_loss 3.412642240524292!\n",
      "9000/11838 Training....batch_loss 3.346898078918457!\n",
      "10000/11838 Training....batch_loss 3.436521291732788!\n",
      "11000/11838 Training....batch_loss 3.3539044857025146!\n",
      "Epoch 49/50 loss 3.30590558052063\n",
      "0/11838 Training....batch_loss inf!\n",
      "1000/11838 Training....batch_loss 2.9600210189819336!\n",
      "2000/11838 Training....batch_loss 3.4713029861450195!\n",
      "3000/11838 Training....batch_loss 3.2860827445983887!\n",
      "4000/11838 Training....batch_loss 3.0829732418060303!\n",
      "5000/11838 Training....batch_loss 3.198060989379883!\n",
      "6000/11838 Training....batch_loss 3.1878514289855957!\n",
      "7000/11838 Training....batch_loss 3.3526506423950195!\n",
      "8000/11838 Training....batch_loss 3.411008358001709!\n",
      "9000/11838 Training....batch_loss 3.345301389694214!\n",
      "10000/11838 Training....batch_loss 3.435102939605713!\n",
      "11000/11838 Training....batch_loss 3.3526124954223633!\n",
      "Epoch 50/50 loss 3.3046610355377197\n",
      "LAST SAVE MODEL EPOCH 50\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "abc11205188759b51cd900f51671874b68e890cecb16a9f92aeb078d6c05e8b5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}